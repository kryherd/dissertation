---
title: "Category Learning Data Processing"
author: "Kayleigh Ryherd"
date: "11/13/2018"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

Read in data, load libraries, set working directory.

```{r load_data}
library(Rmisc)
library(tidyverse)
library(neuropsychology)
library(lme4)
library(lmerTest)
library(fBasics)
library(caret)
library(multcomp)
library(picante)
library(ggcorrplot)
library(multcomp)
setwd("~/dissertation/DataAnalysis/Exp2/CatLearning")
ashby <- read.csv("Ashby_concat_12-07-18_1019.csv")
sloutsky <- read.csv("Sloutsky_concat_12-07-18_1019.csv")
taxthem <- read.csv("taxthem_concat_12-07-18_1020.csv")
age_gender <- read.csv("../age_gender.csv")
```

# Processing/Standardizing Ashby Data

```{r}
# check block names
levels(as.factor(ashby$Block))
# edit typo
ashby$Block[ashby$Block == 33] <- 3
# create across-block trial value
ashby$Overall_Trial <- (ashby$Block-1)*80 + ashby$Trial
# select only blocks 1 - 3
ashby_filt <- dplyr::filter(ashby, Block <= 3)
```

Check to see which subjects have missing blocks & remove those subjects. This means any subject with less than 240 trials (3 full blocks) is dropped.

```{r}
complete <- ashby_filt %>% 
  group_by(Subject, Type) %>% 
  summarise(count = n()) %>% 
  dplyr::filter(count == 240) %>%
  spread(key = Type, value = count) %>%
  na.omit
nrow(complete)
complete_ashby_blocks <- complete$Subject
ashby_full <- ashby_filt %>%
  dplyr::filter(Subject %in% complete_ashby_blocks)
```

## Accuracy

Look at data by condition.

```{r}
ashby_acc_long <- ashby_full %>%
  group_by(Subject, Type) %>%
  summarise(m_acc = mean(Accuracy))

ashby_acc_wide <- spread(ashby_acc_long, Type, m_acc)
ggplot(ashby_acc_wide, aes(RB, II)) + geom_point() + theme_bw() + geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Relationship Between Ashby Conditions - Accuracy")

cor.test(ashby_acc_wide$RB, ashby_acc_wide$II)

ashby_acc_plot <- summarySE(ashby_acc_long, "m_acc", "Type")
ggplot(ashby_acc_long, aes(Type, m_acc)) + geom_violin() + 
  geom_point(aes(y = m_acc), size = 2, position = position_dodge((width = 0.90)), data = ashby_acc_plot) +
  geom_errorbar(aes(ymin = m_acc-se, ymax = m_acc+se), width = 0.20, position = position_dodge((width = 0.90)), data = ashby_acc_plot) +
  theme_bw() + xlab("Condition") + ylab("Accuracy") + ggtitle("Ashby Performance")
```

Look at data by block and condition.

```{r}
ashby_acc_block <- ashby_full %>%
  group_by(Subject, Type, Block) %>%
  summarise(m_acc = mean(Accuracy))
ashby_acc_block$Block <- as.factor(ashby_acc_block$Block)

ashby_acc_plot <- summarySE(ashby_acc_block, "m_acc", c("Type", "Block"))
ggplot(ashby_acc_block, aes(Block, m_acc)) + geom_violin() +
  geom_point(aes(y = m_acc), size = 2, position = position_dodge((width = 0.90)), data = ashby_acc_plot) +
  geom_errorbar(aes(ymin = m_acc-se, ymax = m_acc+se), width = 0.20, position = position_dodge((width = 0.90)), data = ashby_acc_plot) +
  facet_grid(.~Type) +
  theme_bw() + xlab("Block") + ylab("Accuracy") + ggtitle("Ashby Performance by Block")
```

Create *z*-scores for accuracy.

```{r}
ashby_acc_long$acc_z <- scale(ashby_acc_long$m_acc)
ashby_acc_long$System <- NA
ashby_acc_long$System[ashby_acc_long$Type == "II"] <- "associative"
ashby_acc_long$System[ashby_acc_long$Type == "RB"] <- "hypothesis-testing"
ashby_acc_long$CatLearn <- "ashby"
ashby_acc_long <- ashby_acc_long[,-c(2,3)]
```

## RT

Look at data by condition.

```{r}
ashby_rt_long <- ashby_full %>%
  dplyr::filter(Accuracy == 1 & RT > 0.250) %>% # filtering only correct trials and removing RTs faster than 250ms
  group_by(Subject, Type) %>%
  summarise(m_rt = mean(RT))

ashby_rt_wide <- spread(ashby_rt_long, Type, m_rt)
ggplot(ashby_rt_wide, aes(RB, II)) + geom_point() + theme_bw() + geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Relationship Between Ashby Conditions - RT")
```

One subject has weirdly long RTs. I'm going to remove them from the analysis.

```{r}
ashby_rt_long <- ashby_rt_long %>%
  dplyr::filter(Subject != 8088)

ashby_rt_wide <- ashby_rt_wide %>%
  dplyr::filter(Subject != 8088)

ggplot(ashby_rt_wide, aes(RB, II)) + geom_point() + theme_bw() + geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Relationship Between Ashby Conditions - RT")

cor.test(ashby_rt_wide$RB, ashby_rt_wide$II)

ashby_rt_plot <- summarySE(ashby_rt_long, "m_rt", "Type")
ggplot(ashby_rt_long, aes(Type, m_rt)) + geom_violin() + 
  geom_point(aes(y = m_rt), size = 2, position = position_dodge((width = 0.90)), data = ashby_rt_plot) +
  geom_errorbar(aes(ymin = m_rt-se, ymax = m_rt+se), width = 0.20, position = position_dodge((width = 0.90)), data = ashby_rt_plot) +
  theme_bw() + xlab("Condition") + ylab("RT") + ggtitle("Ashby Performance")
```

Look at data by block and condition.

```{r}
ashby_rt_block <- ashby_full %>%
  dplyr::filter(Accuracy == 1, RT > 0.250) %>% 
  group_by(Subject, Type, Block) %>%
  summarise(m_rt = mean(RT))
ashby_rt_block$Block <- as.factor(ashby_rt_block$Block)

ashby_rt_plot <- summarySE(ashby_rt_block, "m_rt", c("Type", "Block"))
ggplot(ashby_rt_block, aes(Block, m_rt)) + geom_violin() +
  geom_point(aes(y = m_rt), size = 2, position = position_dodge((width = 0.90)), data = ashby_rt_plot) +
  geom_errorbar(aes(ymin = m_rt-se, ymax = m_rt+se), width = 0.20, position = position_dodge((width = 0.90)), data = ashby_rt_plot) +
  facet_grid(.~Type) +
  theme_bw() + xlab("Block") + ylab("RT") + ggtitle("Ashby Performance by Block")
```

Create *z*-scores for accuracy.

```{r}
ashby_rt_long$rt_z <- scale(ashby_rt_long$m_rt)
ashby_rt_long$System <- NA
ashby_rt_long$System[ashby_rt_long$Type == "II"] <- "associative"
ashby_rt_long$System[ashby_rt_long$Type == "RB"] <- "hypothesis-testing"
ashby_rt_long$CatLearn <- "ashby"
ashby_rt_long <- ashby_rt_long[,-c(2:3)]
```


# Processing/Standardizing Sloutsky Data

## Accuracy

```{r}
# label each type of response to create d'
sloutsky$response_type[sloutsky$StimType == "target" & sloutsky$RESP == "target"] <- "Hit"
sloutsky$response_type[sloutsky$StimType == "target" & sloutsky$RESP == "nottarget"] <- "Miss"
sloutsky$response_type[sloutsky$StimType == "nottarget" & sloutsky$RESP == "nottarget"] <- "CorRej"
sloutsky$response_type[sloutsky$StimType == "nottarget" & sloutsky$RESP == "target"] <- "FalseAlarm"
sloutsky$response_type[substr(sloutsky$Stimulus,1,5) == "catch" & sloutsky$Accuracy == 1] <- "CatchGood"

# count them
counts <- plyr::count(sloutsky, c('Subject', 'Block', 'response_type'))

# spread them
slout_counts <- sloutsky %>%
  group_by(Subject, Block, response_type) %>%
  count() %>%
  spread(key = response_type, value = n) 

slout_counts[is.na(slout_counts)] <- 0

# create d'
slout_dpr <- slout_counts %>%
  mutate(dpr = dprime(Hit, Miss, FalseAlarm, CorRej)$dprime)

sl_plot_acc <- summarySE(slout_dpr, "dpr", "Block")
ggplot(slout_dpr, aes(Block, dpr)) + geom_violin() + 
  geom_point(aes(y = dpr), size = 2, position = position_dodge((width = 0.90)), data = sl_plot_acc) +
  geom_errorbar(aes(ymin = dpr-se, ymax = dpr+se), width = 0.20, position = position_dodge((width = 0.90)), data = sl_plot_acc) +
  theme_bw() + xlab("Block") + ylab("Accuracy (d')") + ggtitle("Sloutsky Performance")
```

Create *z*-scores for accuracy.

```{r}
slout_acc <- slout_dpr %>%
  dplyr::filter(Block == "SupervisedSparse" | Block == "UnsupervisedDense") %>%
  dplyr::select(Subject, Block, dpr)
slout_acc$acc_z <- scale(slout_acc$dpr)
slout_acc$System <- NA
slout_acc$System[slout_acc$Block == "UnsupervisedDense"] <- "associative"
slout_acc$System[slout_acc$Block == "SupervisedSparse"] <- "hypothesis-testing"
slout_acc$CatLearn <- "sloutsky"
slout_acc <- slout_acc[,-c(2,3)]
```

## RT

```{r}
# recode non-answers
sloutsky$RT[sloutsky$RT == 9999] <- NA
sl_rt <- sloutsky %>%
  dplyr::filter(Accuracy == 1 & RT > 0.250) %>%
  group_by(Subject, Block) %>%
  summarise(m_RT = mean(RT, na.rm = TRUE))

sl_plot_rt <- summarySE(sl_rt, "m_RT", "Block", na.rm = TRUE)
ggplot(sl_rt, aes(Block, m_RT)) + geom_violin() + 
  geom_point(aes(y = m_RT), size = 2, position = position_dodge((width = 0.90)), data = sl_plot_rt) +
  geom_errorbar(aes(ymin = m_RT-se, ymax = m_RT+se), width = 0.20, position = position_dodge((width = 0.90)), data = sl_plot_rt) +
  theme_bw() + xlab("Block") + ylab("RT") + ggtitle("Sloutsky Performance")
```

Comparing SS to UD. It doesn't look like ACC is helpful here.

```{r}
sl_filt <- slout_dpr %>% 
  dplyr::filter(Block %in% c("SupervisedSparse","UnsupervisedDense")) %>%
  dplyr::select(Subject, Block, dpr)
sl_wide <- spread(sl_filt, Block, dpr)
ggplot(sl_wide, aes(UnsupervisedDense, SupervisedSparse)) + geom_point() + theme_bw() + 
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Relationship Between Sloutsky Conditions - ACC")

sl_filt <- dplyr::filter(sl_rt, Block %in% c("SupervisedSparse","UnsupervisedDense"))
sl_wide <- spread(sl_filt, Block, m_RT)
ggplot(sl_wide, aes(UnsupervisedDense, SupervisedSparse)) + geom_point() + theme_bw() + 
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Relationship Between Sloutsky Conditions - RT")
```

Standardizing Sloutsky RTs.

```{r}
sl_filt$rt_z <- scale(sl_filt$m_RT)
sl_filt$System <- NA
sl_filt$System[sl_filt$Block == "UnsupervisedDense"] <- "associative"
sl_filt$System[sl_filt$Block == "SupervisedSparse"] <- "hypothesis-testing"
sl_filt$CatLearn <- "sloutsky"
sl_filt <- sl_filt[,-c(2,3)]
```

# Processing/Standardizing Tax/Them Data

## Accuracy

```{r}
test_trials <- dplyr::filter(taxthem, !is.na(expTrials.thisTrialN))

tt_acc <- test_trials %>%
  group_by(participant, Experiment) %>%
  summarise(m_acc = mean(resp.corr))

tt_plot_acc <- summarySE(tt_acc, "m_acc", "Experiment")
ggplot(tt_acc, aes(Experiment, m_acc)) + geom_violin() + 
  geom_point(aes(y = m_acc), size = 2, position = position_dodge((width = 0.90)), data = tt_plot_acc) +
  geom_errorbar(aes(ymin = m_acc-se, ymax = m_acc+se), width = 0.20, position = position_dodge((width = 0.90)), data = tt_plot_acc) +
  theme_bw() + xlab("Condition") + ylab("Accuracy") + ggtitle("Tax/Them Performance")

tt_acc_wide <- tt_acc %>%
  group_by(participant) %>%
  spread(key = Experiment, value = m_acc) 

ggplot(tt_acc_wide, aes(taxonomic, thematic)) + geom_point() + geom_smooth(se = FALSE, method = "lm") + theme_bw()

cor.test(tt_acc_wide$taxonomic, tt_acc_wide$thematic)  
```

Create *z*-scores for accuracy.

```{r}
tt_acc$acc_z <- scale(tt_acc$m_acc)
tt_acc$System <- NA
tt_acc$System[tt_acc$Experiment == "taxonomic"] <- "hypothesis-testing"
tt_acc$System[tt_acc$Experiment == "thematic"] <- "associative"
tt_acc$CatLearn <- "tt"
tt_acc <- tt_acc[,-c(2,3)]
colnames(tt_acc)[1] <- "Subject"
```

## RT

```{r}
# remove RTs that are too short (less than 250ms), incorrect trials
cor_trials <- dplyr::filter(test_trials, resp.corr == 1 & resp.rt > 0.250)

tt_rt <- cor_trials %>%
  group_by(participant, Experiment) %>%
  summarise(m_RT = mean(resp.rt))

tt_plot_rt <- summarySE(tt_rt, "m_RT", "Experiment")
ggplot(tt_rt, aes(Experiment, m_RT)) + geom_violin() + 
  geom_point(aes(y = m_RT), size = 2, position = position_dodge((width = 0.90)), data = tt_plot_rt) +
  geom_errorbar(aes(ymin = m_RT-se, ymax = m_RT+se), width = 0.20, position = position_dodge((width = 0.90)), data = tt_plot_rt) +
  theme_bw() + xlab("Condition") + ylab("RT") + ggtitle("Tax/Them Performance")

tt_wide <- spread(tt_rt, Experiment, m_RT)
ggplot(tt_wide, aes(taxonomic, thematic)) + geom_point() + theme_bw() + 
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Relationship Between TaxThem Conditions - RT")

tt_rt$rt_z <- scale(tt_rt$m_RT)
tt_rt$System <- NA
tt_rt$System[tt_rt$Experiment == "taxonomic"] <- "hypothesis-testing"
tt_rt$System[tt_rt$Experiment == "thematic"] <- "associative"
tt_rt$CatLearn <- "tt"
colnames(tt_rt)[1] <- "Subject"
tt_rt <- tt_rt[,-c(2,3)]
```

# Combining the 3 experiments

## Accuracy

```{r}
# binding the data frames together
acc_long <- dplyr::bind_rows(ashby_acc_long, slout_acc, tt_acc)
acc_wide <- acc_long %>%
  spread(key = CatLearn, value = acc_z) %>%
  na.omit

# find which subjects have full data for both systems and only use those subjects
both_systems <- acc_wide$Subject[duplicated(acc_wide$Subject)]
acc_wide <- dplyr::filter(acc_wide, Subject %in% both_systems)

# test the normality of the different variables
dagoTest(acc_wide$ashby)
dagoTest(acc_wide$sloutsky)
dagoTest(acc_wide$tt)

# sloutsky and tt are skewed, so let's transform
vars_tf <- c("sloutsky", "tt")
# transformations
pp_md_tf <- caret::preProcess(as.data.frame(acc_wide[,vars_tf]), method = c("center", "scale", "YeoJohnson"), na.remove=T)
tf_data <- predict(pp_md_tf, as.data.frame(acc_wide[,vars_tf]))
# re-check normality
dagoTest(tf_data$ashby)
dagoTest(tf_data$sloutsky)
dagoTest(tf_data$tt)
# less skewed at least

# move data into main data frame
acc_wide$sloutsky.t <- tf_data$sloutsky
acc_wide$tt.t <- tf_data$tt
# center and scale ashby (because we did that with the tf data)
acc_wide$ashby.cs <- scale(acc_wide$ashby)

write.csv(acc_wide, "catlearn_proc_acc.csv")

# merge in age and gender
acc_demo <- acc_wide %>%
  inner_join(.,age_gender, by = "Subject")

# only select first 84 subjects (per preregistration)
acc_sel <- acc_demo[c(1:(2*84)),]


acc_sel$Gender[acc_sel$Gender == "female"] <- "Female"
acc_sel$Gender[acc_sel$Gender == "male"] <- "Male"
# demographics for this sample
acc_sel %>%
  distinct(Subject, Age, Gender) %>%
  group_by(Gender) %>%
  dplyr::count(Gender)

acc_sel %>%
  distinct(Subject, Age, Gender) %>%
  ungroup() %>%
  summarise(m_age = mean(Age),
            sd_age = sd(Age),
            min_age = min(Age),
            max_age = max(Age))

# select only the hypothesis testing data and see their correlation
ht <- subset(acc_sel, System == "hypothesis-testing")
cor.mat <- cor(ht[,c(6:8)])
p.mat <- cor_pmat(ht[,c(6:8)])
ggcorrplot(cor.mat, type = "lower", p.mat = p.mat, lab = TRUE, title = "ACC - Hypothesis-testing")

# same with associative
as <- subset(acc_sel, System == "associative")
cor(as[,c(6:8)])
cor.mat <- cor(as[,c(6:8)])
p.mat <- cor_pmat(as[,c(6:8)])
ggcorrplot(cor.mat, type = "lower", p.mat = p.mat, lab = TRUE, title = "ACC - Associative")

# Plot accuracy
acc_sel_long <- acc_sel %>%
  dplyr::select(Subject, System, sloutsky.t:ashby.cs) %>%
  gather(key ="CatLearn", value = "acc_z", sloutsky.t:ashby.cs)

# by system
acc_all_plot <- summarySE(acc_sel_long, "acc_z", c("System", "CatLearn"))
ggplot(acc_sel_long, aes(CatLearn, acc_z)) + geom_violin() + 
  geom_point(aes(y = acc_z), size = 2, position = position_dodge((width = 0.90)), data = acc_all_plot) +
  geom_errorbar(aes(ymin = acc_z-se, ymax = acc_z+se), width = 0.20, position = position_dodge((width = 0.90)), data = acc_all_plot) +
  facet_grid(.~System) +
  theme_bw() + xlab("Experiment") + ylab("Accuracy Z-Score (transformed)") + ggtitle("")

# by paradigm
ggplot(acc_sel_long, aes(System, acc_z)) + geom_violin() + 
  geom_point(aes(y = acc_z), size = 2, position = position_dodge((width = 0.90)), data = acc_all_plot) +
  geom_errorbar(aes(ymin = acc_z-se, ymax = acc_z+se), width = 0.20, position = position_dodge((width = 0.90)), data = acc_all_plot) +
  facet_grid(.~CatLearn) +
  theme_bw() + xlab("Experiment") + ylab("Accuracy Z-Score (transformed)") + ggtitle("")

# run basic models
m1 <- lmer(acc_z ~ CatLearn * System + (1|Subject), data = acc_sel_long)
anova(m1)

# break down interaction between paradigm and system
HTSubset <- acc_sel_long$System=="hypothesis-testing"
ASSubset <- acc_sel_long$System=="associative"

# check hypothesis testing first
HTmodel <- lmer(acc_z ~ CatLearn + (1|Subject), data = acc_sel_long, subset = HTSubset)
summary(HTmodel)
anova(HTmodel)
# look at post hocs
summary(glht(HTmodel, linfct = mcp(CatLearn = "Tukey")), test = adjusted("holm"))
```

Basically, this is saying that Sloutsky and TT lower (relative) accuracy than Ashby (when standardized).


```{r}
# now check associative
ASmodel <- lmer(acc_z ~ CatLearn + (1|Subject), data = acc_sel_long, subset = ASSubset)
summary(ASmodel)
anova(ASmodel)
# check post-hocs
summary(glht(ASmodel, linfct = mcp(CatLearn = "Tukey")), test = adjusted("holm"))
```

Here we see the opposite. Sloutsky and TT have higher (relative) accuracy than Ashby. I think these results generally reflect that the difference in ACC between systems is much larger in Ashby than in Sloutsky or TT. Also, Sloutsky and TT show a trend (at least) towards higher accuracy in associative, while Ashby shows that HT is definitely higher accuracy.

## RT

```{r}
# bring together RT data
rt_long <- dplyr::bind_rows(ashby_rt_long, sl_filt, tt_rt)

# make a wide dataset
rt_wide <- rt_long %>%
  spread(key = CatLearn, value = rt_z) %>%
  na.omit

# find which subjects have both systems and keep those
both_systems <- rt_wide$Subject[duplicated(rt_wide$Subject)]
rt_wide <- dplyr::filter(rt_wide, Subject %in% both_systems)

# check normality
dagoTest(rt_wide$ashby)
dagoTest(rt_wide$sloutsky)
dagoTest(rt_wide$tt)

# all are very skewed. transform.
vars_tf <- c("ashby", "sloutsky", "tt")
pp_md_tf <- preProcess(as.data.frame(rt_wide[,vars_tf]), method = c("center", "scale", "YeoJohnson"), na.remove=T)
tf_data <- predict(pp_md_tf, as.data.frame(rt_wide[,vars_tf]))

# re-check normality
dagoTest(tf_data$ashby)
dagoTest(tf_data$sloutsky)
dagoTest(tf_data$tt)
# way better!

# add to main dataframe
rt_wide$ashby.t <- tf_data$ashby
rt_wide$sloutsky.t <- tf_data$sloutsky
rt_wide$tt.t <- tf_data$tt

write.csv(rt_wide, "catlearn_proc_rt.csv")

# select only first 84 subjects (per preregistration)
rt_sel <- rt_wide[c(1:(2*84)),]

# look at correlations
# first for hypothesis-testing
ht <- subset(rt_sel, System == "hypothesis-testing")
cor.mat <- cor(ht[,c(6:8)])
p.mat <- cor_pmat(ht[,c(6:8)])
ggcorrplot(cor.mat, type = "lower", p.mat = p.mat, lab = TRUE, title = "RTs - Hypothesis-testing")

# now hypothesis testing
as <- subset(rt_sel, System == "associative")
cor.mat <- cor(as[,c(6:8)])
p.mat <- cor_pmat(as[,c(6:8)])
ggcorrplot(cor.mat, type = "lower", p.mat = p.mat, lab = TRUE, title = "RTs - Associative")

rt_sel_long <- rt_sel %>%
  dplyr::select(Subject, System, ashby.t:tt.t) %>%
  gather(key ="CatLearn", value = "rt_z", ashby.t:tt.t)

# now plot
# first by system
rt_all_plot <- summarySE(rt_sel_long, "rt_z", c("System", "CatLearn"))
ggplot(rt_sel_long, aes(CatLearn, rt_z)) + geom_violin() + 
  geom_point(aes(y = rt_z), size = 2, position = position_dodge((width = 0.90)), data = rt_all_plot) +
  geom_errorbar(aes(ymin = rt_z-se, ymax = rt_z+se), width = 0.20, position = position_dodge((width = 0.90)), data = rt_all_plot) +
  facet_grid(.~System) +
  theme_bw() + xlab("Experiment") + ylab("RT Z-Score (transformed)") + ggtitle("")

# then by paradigm
ggplot(rt_sel_long, aes(System, rt_z)) + geom_violin() + 
  geom_point(aes(y = rt_z), size = 2, position = position_dodge((width = 0.90)), data = rt_all_plot) +
  geom_errorbar(aes(ymin = rt_z-se, ymax = rt_z+se), width = 0.20, position = position_dodge((width = 0.90)), data = rt_all_plot) +
  facet_grid(.~CatLearn) +
  theme_bw() + xlab("Experiment") + ylab("RT Z-Score (transformed)") + ggtitle("")

m1 <- lmer(rt_z ~ CatLearn * System + (1|Subject), data = rt_sel_long)
anova(m1)

# check that ineraction
HTSubset <- rt_sel_long$System=="hypothesis-testing"
ASSubset <- rt_sel_long$System=="associative"

# check hypothesis testing first
HTmodel <- lmer(rt_z ~ CatLearn + (1|Subject), data = rt_sel_long, subset = HTSubset)
summary(HTmodel)
anova(HTmodel)
# look at post hocs
summary(glht(HTmodel, linfct = mcp(CatLearn = "Tukey")), test = adjusted("holm"))

# only difference is that tt z-scores are a little higher than ashby z-scores

# now check associative
ASmodel <- lmer(rt_z ~ CatLearn + (1|Subject), data = rt_sel_long, subset = ASSubset)
summary(ASmodel)
anova(ASmodel)
# check post-hocs
summary(glht(ASmodel, linfct = mcp(CatLearn = "Tukey")), test = adjusted("holm"))

# no differences here

```





