\documentclass[../dissertation.tex]{subfiles}
 
\begin{document}

\section{Experiment 2}

	This experiment tests the core hypothesis of this dissertation. Namely, I use a within-subjects design to test whether executive function is specifically related to categorization in the hypothesis-testing system while verbal labels are specifically related to associative system categorization. This experiment also uses three different category learning tasks, allowing me compare different category learning paradigms and to test the core hypothesis for multiple approaches. \par
	In this experiment, I use vocabulary as a proxy for labeling. Vocabulary measures should reflect the link between a word and its meaning. Further, this link should be more elaborated than those built in a paired-associate learning task. Thus, individual differences in vocabulary should give some insight into how well participants use labels in learning categories. For executive function, I selected three different measures. Multiple studies have shown that while executive function is sometimes talked about as a single construct, it actually is made up of separable components \citep{Miyake2000, Karr2018}. The components I chose to focus on were inhibition, switching, and planning. \par
	I chose to compare the COVIS, statistical density, and taxonomic-thematic approaches to category learning. These three approaches represent a broad spectrum of category learning. COVIS exclusively focuses on perceptual categories, while the statistical density approach uses constructed stimuli that can be mapped onto real-world objects at least somewhat. Finally, the taxonomic-thematic approach is almost always applied to real-world objects with which participants have experience. Since these three approaches are so different in the types of categories they try to explain, results showing similarities between them would be strong evidence for an overarching dual-systems framework. I decided to use common paradigms from each approach as a starting point for comparison. Given the theoretical similarities between the approaches, I hypothesize that task differences will not be sufficient to affect the relationship between the two systems differently for each approach.

\subsection{Method}
\subsubsection{Participants}
XX participants were recruited from the psychology undergraduate participant pool at the University of Connecticut (X Female, X Male, mean age = X).
\subsubsection{Category Learning Tasks}
This experiment used three different category learning tasks, each based on a different approach to category learning. I used these three tasks to investigate whether the paradigms used in different approaches engage category learning systems in a similar way. The order of category learning tasks was counterbalanced across participants. All category learning tasks were presented using PsychoPy v.1.84.2 \citep{Peirce2007}. \par 
\textbf{Sloustky statistical density task.} This task used the same procedure and stimuli as the task described in Experiment 1. However, instead of completing only two blocks, participants completed all four blocks. Because the previous experiment showed few significant order effects, the order of the four blocks was randomly generated for each participant. \par
\textbf{Ashby perceptual category learning task.} There were two versions to this task: Information-Integration (II) and Rule-Based (RB). Participants completed the II version and then the RB version. Prior research has shown that when participants are asked to switch between the declarative (hypothesis-testing) and implicit (associative) systems, they end up using rule-based strategies from the declarative system for all trials. Thus, by engaging the implicit system first, I aimed to reduce transfer effects between versions as much as possible. \par
In each version of the task, participants were told that they would be learning two categories and that perfect performance was possible. They were also told to be as quick and accurate as possible. In each trial, participants viewed a Gabor patch that belonged to one of the two categories. Each patch subtended 11\degree  of visual angle. The stimuli were generated using category parameters from \citet{Maddox2003}. The participant then had 5000ms to press a key, indicating which category they believed the stimulus belonged to. After a response, the participant received feedback ("Correct" or "Incorrect"). Feedback was presented for 1000ms, and then the next trial began. If the participant took more than 5000ms to respond, they saw "Too Slow" and proceeded to the next trial. Participants completed five runs of each version. Each run had 80 trials (40 from each category) presented in a random order. Thus, in total participants completed 240 II trials and 240 RB trials. \par
\textbf{Taxonomic/thematic task.} This task was adapted from \citet{Murphy2001} and \citet{Kalenine2009}. There were also two versions of this task: one taxonomic and one thematic. Version order was counterbalanced across subjects, with some participants getting the taxonomic version first and others the thematic version first. Most versions of this type of task allow participants to choose the item that is most "semantically related," and thus do not ask participants to make either taxonomic or thematic choices on any given trial. As such, little research has looked at switching between taxonomic and thematic semantic judgments. Counterbalancing was applied to control for order effects. \par
The stimuli were images taken from \citet{Konkle2010}. I chose to use images in order to avoid automatic language processing. While participants likely did engage linguistic resources during the task, the use of pictures reduced the influence of stimulus features on language use. In each trial, four images were presented: a target, a taxonomically-related item, a thematically-related item, and an unrelated item. Taxonomically- and thematically-related items were chosen based on norms from \citet{Landrigan2016} where available. The \citet{Landrigan2016} norms were based on word stimuli rather than the images available from \citet{Konkle2010}; as such, not all of the available images were normed. For images without norming information, researcher judgment was used to pick items for each type of relation. \par
For each version, participants were told that they would be categorizing objects. They were told to pick the option that "goes best with" (thematic) or is "most similar to" (taxonomic) the target item. These instructions were based on previous research showing that slight differences in task instructions affect taxonomic and thematic judgments \citep{Lin2001}. After instructions, participants got five practice trials. In each trial, the images were shown for 5000ms and participants had unlimited time to make a response. The practice trials were identical for the taxonomic and thematic versions of the task. After each response, participants received feedback ("Correct!" or "Oops!") for 1000ms. Once the practice trials were completed, participants received 24 test trials. While some images were seen in multiple trials, the 4-image combination for each trial was unique across the taxonomic and thematic versions of the task.
\subsubsection{Executive Function Tasks} 
To measure executive function, I used three different tasks taken from the Psychology Experiment Building Language (PEBL) test battery \citep{Mueller2014}. I chose three tasks to try and tap multiple aspects of executive function, including inhibition, planning, and task-switching. All three tasks were presented using the PEBL software.\par
\textbf{Flanker task (inhibition).} This task was an implementation of the \citet{Eriksen1979} flanker task, using a method similar to \citet{Stins2007}. In each trial, participants viewed a set of five arrows and were asked to respond based on the direction in which the center arrow was pointing (left or right). In congruent trials, all arrows faced the same way. In incongruent trials, the four distractor arrows pointed in the opposite direction of the target (center) arrow. In neutral trials, the four distractor arrows were just horizontal lines without arrowheads. In blank trials, only the target arrow appeared. Participants completed 40 trials in a 2 (direction; left/right) x 4 (condition; congruent, incongruent, neutral, blank) design, for a total of 160 trials. Each trial began with a 500 ms fixation, followed by the stimulus which appeared for 800ms. Participants were only allowed to respond during the 800ms that the stimulus was on the screen. After a response, there was an inter-trial interval of 1000ms. Participants received 8 practice trials before the actual experiment to get used to the timing of each trial. During practice trials, each response was followed by feedback ("Correct", "Incorrect") as well as a number indicating RT for that trial. This feedback was not provided for the test trials. \par
\textbf{Switcher task (task-switching).} This task was taken from \citet{Anderson2012}. In this task, participants were presented with an array of colored shapes. Each colored shape had a single letter inside. For each trial, a single shape was indicated to be the target shape. Based on instructions at the top of the screen, participants were told to select a new shape that matched the target shape on one of three dimensions (color, shape, or letter). Research from \citet{Miyake2004} has shown that cueing a dimension using its entire name (e.g. "shape") does not require as many language resources as cueing a dimension using a single letter (e.g., "s"). Since one of the core hypotheses of this study was that language supports executive functions in the hypothesis-testing system, I used a version of the switcher task that cued dimension using just a single letter. I expect that this version of the task requires individuals to represent dimensions/selection rules internally, similar to how they might represent possible category rules when learning rule-based categories. \par
The task consisted of nine different arrays of ten shapes. For each array, participants made ten responses. In the first three arrays, participants switched between two of the three dimensions in a fixed order (e.g., C - S - C - S, etc.). The relevant dimensions were different for each array. For the second three arrays, participants switched between all three dimensions still in a fixed order (e.g., S - C - L - S - C - L, etc). The specific order was different for each array. Finally, in the last three arrays participants switched between all three dimensions in a random order. Unlike previous arrays, in the last three participants were unable to anticipate the upcoming relevant dimension. \par
\textbf{Tower of London task (planning)}. This task was a computerized version of the one described in \citet{Shallice1982}. In this task, participants were shown a setup of colored disks in three stacks as well as a target setup. They were given a limited number of moves to make their setup match the target setup. Participants could only have one disk in their "hand" at a time, and they could only pick the top disk up off of any stack. The trials varied in the number of steps required to match the target setup from 2 to 5, with easier (2 step) trials at the beginning of the task and harder (5 step) trials at the end of the task. Participants were encouraged to take their time and plan out their moves before beginning each trial. 

\subsubsection{Behavioral Measures} 
Finally, I used four different behavioral assessments to measure vocabulary, syntax, and nonverbal IQ.
\textbf{Nelson-Denny vocabulary subtest.} To measure vocabulary, U used the same Nelson-Denny vocabulary subtest described in experiment 1. \par
\textbf{Clinical Evaluation of Language Fundamentals recalling sentences and formulated sentences subtests}. I used the CELF here to measure individual differences in syntax production and perception. The Recalling Sentences subtest provided a measure of receptive grammar, while the Formulated Sentences subtest measured expressive grammar. In the Formulated Sentences subtest, participants viewed a scene and were asked to make a sentence containing a target word about that scene. Often, the target word encouraged certain syntactic structures (e.g., "because"). \par
\textbf{Raven's Advanced Matrices.} I used Raven's Advanced matrices to measure nonverbal IQ, as described in Experiment 1.

\subsubsection{Procedure}

Each participant completed all of the category learning and executive function tasks, as well as all of the behavioral measures. CELF responses were audio-recorded to allow for offline scoring. To allow multiple subjects to be run in a single timeslot, some participants received tasks in a shuffled order. All together, the tasks and behavioral measures took about an hour and a half.

\subsection{Results I: Individual Differences}

Descriptive statistics for all individual difference measures can be found in Table TABLE HERE.

\subsubsection{Data Processing}
	An \textit{a priori} power analysis showed that 132 subjects would be needed for this experiment. As is noted above, more than 132 subjects participated in the experiment. Due to the many assessments being collected, issues with the experimental paradigm, and experimental time constraints, there was a considerable amount of data missingness. Thus, each analysis discussed below uses the first 132 subjects with full data for all measures included in that analysis. \par
	\textbf{Ashby perceptual category learning task.} For this task, II blocks were labeled as associative and RB as hypothesis-testing. Accuracy and reaction time were measured for this task. Accuracy was summarized by subject and system. For reaction time, only accuracy trials were used. Then, outliers were removed on a by-trial basis by calculating the mean and standard deviation of reaction times within a given subject and system. Any trial with a reaction time more than 2 SDs away from the mean was discarded. Then, reaction time was summarized by subject and system. A single subject had reaction times 8 SD higher than the mean; this subject's data was removed and replaced with another subject. Accuracy and reaction time were then Yeo-Johnson transformed to reduce skewness, as well as centered and scaled. \par
	\textbf{Sloutsky statistical density task.} In this task, the unsupervised-dense block was considered to engage the associative system, and the supervised-sparse block was considered to engage the hypothesis-testing system. The other two blocks were discarded. Any participants who did not respond correctly to at least 6 of the 8 catch trials for a given block were removed from future analyses. Thus, all subjects reported in analyses using this task had at least 75\% accuracy on catch trials in both blocks. Accuracy was summarized by subject and system. Reaction time outliers were removed on a by-trial basis as described above and reaction time was then summarized by subject and system. Accuracy and reaction time were then transformed, centered, and scaled. \par
	\textbf{Taxonomic/thematic task.} For this task, taxonomic blocks were associative and thematic blocks were hypothesis-testing. Practice trials were discarded before analysis. Accuracy was summarized by subject and system. Reaction time outliers were removed using the same method as above, and then reaction time was summarized by subject and system. Accuracy and reaction time were then transformed, centered, and scaled. \par
	\textbf{Flanker task.} The flanker effect was calculated by first selecting only incongruent or congruent trials. Then, the average reaction time for each trial type was calculated by subject. Finally, the average reaction time for congruent trials was subtracted from the average reaction time for incongruent trials for each subject. This measure reflecting the flanker effect was then centered and scaled but not transformed. \par
	\textbf{Switcher task.} The switcher effect was calculated by selecting the 3-dimension ordered and 3-dimension random blocks. Average reaction time was calculated for each subject in each of these blocks. Then, the reaction time for 3-dimension ordered was subtracted from the reaction time for 3-dimension random. This measure was then transformed, centered, and scaled. \par 
	\textbf{Tower of London task.} The main metric calculated for this task was the ratio of planning time to total trial time for each subject. Planning time and total trial time were summarized for each subject. Then, planning time was divided by total trial time. This measure was then transformed, centered, and scaled. \par 
	\textbf{Nelson-Denny, CELF, RAM}. Nelson-Denny and CELF were both standardized using their respective norms. Raven's was not standardized. All 4 measures (Nelson-Denny vocabulary, CELF Recalling Sentences, CELF Formulated Sentences, and Raven's Advanced Matrices) were all transformed, centered, and scaled.
	
\subsubsection{Accuracy}

\textbf{Ashby perceptual category learning task.} \par 
\textbf{Sloutsky statistical density task.}  To investigate how the individual difference measures related to task performance, I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system marginally improved model fit,  $\chi^{2}(1)$ = 3.38, p = 0.07. In the next step, I added RAM, vocabulary, and the three executive function tasks as fixed effects. This step significantly improved fit, $\chi^{2}(5)$  = 20.24, \textit{p} = 0.001. However, adding the interactions between system and the individual difference measures (except RAM) did not further improve fit,  $\chi^{2}(4)$ = 3.03, p = 0.55. Thus, the final model predicted accuracy in this task from system and the individual difference measures, but not their interactions. This model revealed a significant main effect of flanker, \textit{F}(1,126) = 4.93, \textit{p} = 0.03, and a significant main effect of switcher, \textit{F}(1,126) = 9.70, \textit{p} = 0.003. The coefficient associated with flanker was positive (\textit{b} = 0.16, \textit{SE} = 0.07), while the coefficient associated with switcher was negative, (\textit{b} = -0.21, \textit{SE} = 0.07). There were also marginally significant effects of system, \textit{F}(1,126) = 3.40, \textit{p} = 0.07, Tower of London, \textit{F}(1,126) = 3.56, \textit{p} = 0.06, and vocabulary, \textit{F}(1,126) = 2.82, \textit{p} = 0.10. Thus, accuracy on the Sloutsky statistical density task was positively related to flanker performance and negatively related to switcher performance. However, as in the prior analysis, these results should be interpreted with caution, as large ceiling effects were found in accuracy for this task. \par
\textbf{Taxonomic/thematic task.}  Again I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system significantly improved model fit,  $\chi^{2}(1)$ = 8.59, p = 0.003. Adding the individual difference measures also significantly improved fit, $\chi^{2}(5)$  = 11.46, \textit{p} = 0.04. However, adding the interactions between system and the individual difference measures (except RAM) did not further improve fit,  $\chi^{2}(4)$ = 1.81, p = 0.77. Thus, the final model predicted accuracy in this task from system and the individual difference measures, but not their interactions. This model revealed a significant main effect of system, \textit{F}(1,131) = 8.80, \textit{p} = 0.003, and a significant main effect of RAM, \textit{F}(1,126) = 4.78, \textit{p} = 0.031. The coefficient associated with RAM was positive (\textit{b} = 0.17, \textit{SE} = 0.08). Thus, accuracy on the taxonomic/thematic task was positively related to performance on RAM. \par
\textbf{Summary.} This section showed that different individual difference measures are related to accuracy in the three category learning tasks. For the Sloutsky statistical density task, accuracy was positively related to flanker performance and negatively related to switcher performance. RAM was positively related to accuracy in the taxonomic/thematic task.

\subsubsection{Reaction Time}

\textbf{Ashby perceptual category learning task.} \par 
\textbf{Sloutsky statistical density task.}  Once again I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system significantly improved model fit,  $\chi^{2}(1)$ = 53.01, p \textless 0.001. Adding the individual difference measures as fixed effects also significantly improved fit, $\chi^{2}(5)$  = 17.62, \textit{p} = 0.003. However, adding the interactions between system and the individual difference measures (except RAM) only marginally improved fit,  $\chi^{2}(4)$ = 8.13, p = 0.09. Thus, the final model predicted accuracy in this task from system and the individual difference measures, but not their interactions. This model revealed a significant main effect of system, \textit{F}(1,131) = 64.75, \textit{p} \textless 0.001, a main effect of RAM, \textit{F}(1,126) = 4.26, \textit{p} = 0.04, and a significant main effect of Tower of London, \textit{F}(1,126) = 9.18, \textit{p} = 0.003. The coefficient associated with RAM was positive (\textit{b} = 0.16, \textit{SE} = 0.08), as was the coefficient associated with Tower of London, (\textit{b} = 0.22, \textit{SE} = 0.07). Thus, participants who took longer to respond in this task also performed better on RAM and spent a larger portion of their total time planning in Tower of London. \par
\textbf{Taxonomic/thematic task.}  I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system significantly improved model fit,  $\chi^{2}(1)$ = 19.70, p \textless 0.001. However, adding the individual difference measures did not further improve fit,  $\chi^{2}(4)$ = 3.89, \textit{p} = 0.42. Indeed, the only effect even close to significant in this model was a marginal main effect of Tower of London, \textit{F}(1,127) = 3.17, \textit{p} = 0.08. Thus, reaction time on the taxonomic/thematic task was not related to any of the individual difference measures.
\textbf{Summary.} Similar to what was seen in accuracy above, this set of analyses shows that different individual difference measures are related to speed during the three different category learning tasks. Participants who responded faster during the Sloutsky task had lower RAM scores and spent less time planning in the Tower of London. Finally, no individual difference measures were related to reaction time in the taxonomic/thematic task.

\subsubsection{Exploratory Analyses}

While this section is largely devoted to testing the relationship between category learning and individual differences in vocabulary and executive function, two additional individual difference measures were collected (CELF RS and FS). Next I will report the results of analyses investigating the relationship between category learning and these measures. \par
\textbf{Ashby perceptual category learning task.} \par
\textbf{Sloutsky statistical density task.} For both accuracy and reaction time, the base model was a mixed-effects model with random intercepts for subject and fixed effects of system and RAM. Adding the two CELF measures did not significantly improve fit for accuracy, $\chi^{2}(2)$ = 2.77, \textit{p} = 0.25, or reaction time, $\chi^{2}(2)$ = 0.51, \textit{p} = 0.78. Thus, CELF FS and RS did not predict accuracy or reaction time in this task. \par
\textbf{Taxonomic/thematic task.} The base model for both accuracy and reaction time had random intercepts for subject and fixed effects for system and RAM. CELF FS and RS did not significantly improve model fit for accuracy, $\chi^{2}(2)$ = 0.12, \textit{p} = 0.94, or reaction time, $\chi^{2}(2)$ = 2.20, \textit{p} = 0.33. \par
\textbf{Summary.} These analyses largely show that receptive and expressive grammar were not related to performance in the three category learning tasks in this study. This may be influenced by the fact that the participants in this study were at the upper age limit for the CELF (18-21 years) and that they represent a typical (non-language-impaired) population. Thus, this measure may not have been sensitive to fine-grained individual differences in receptive and expressive grammar in this population.

\subsection{Discussion I: Individual Differences}

	In this section, I tested the core hypothesis of this dissertation, which was that categorization in the hypothesis-testing system relies on executive function while categorization in the associative system relies on verbal labels. I used three category learning tasks and three executive function tasks in addition to measuring vocabulary, which I used as a measure of labeling ability. I expected to find that executive function was strongly related to categorization across tasks in hypothesis-testing blocks but not associative blocks. Conversely, I expected to see strong relationships between associative block performance and vocabulary, but no relationship between hypothesis-testing block performance and vocabulary. \par 
	These hypotheses were not supported by the data. To begin, only one analysis (Ashby reaction time) showed an interaction between system and any of the individual difference measures. All of the significant predictors for the Sloutsky statistical density and taxonomic/thematic tasks were main effects, suggesting that the contributions of these individual difference measures do not differ by category learning system. Further, different measures were significant predictors for each category learning task. Vocabulary was the strongest predictor for the Ashby perceptual category task. The Sloutsky statistical density task showed significant effects of nonverbal IQ and executive function. Finally, the taxonomic/thematic task only showed effects of nonverbal IQ. \par
	
\subsubsection{Ashby perceptual category learning task}
	The system by vocabulary interaction in the Ashby perceptual category learning task is certainly the most interesting result. There was no relationship between vocabulary and reaction time in the hypothesis-testing block of this task. However, participants with higher vocabularies also showed slower reaction times in the associative block. This is consistent with broad predictions from COVIS; information-integration categories, which are best learned by the associative system, should have no verbal rule for inclusion. Thus, recruiting verbal resources when learning an information-integration category could slow down processing, perhaps by engaging the suboptimal hypothesis-testing system. Indeed, one study showed that when a verbal rule is accessible, the hypothesis-testing system takes over even when it is suboptimal for learning \citep{Noseworthy2011}. Thus, participants with higher vocabularies may recruit verbal, hypothesis-testing resources in associative blocks which slows down processing. However, accessing verbal resources may not affect reaction time in hypothesis-testing blocks since these resources aid in this type of learning. \par
	The lack of significant executive function effects in the Ashby perceptual category task specific to the hypothesis-testing system is not particularly surprising, given the mixed literature on the topic. Some studies do find specific relationships between aspects of executive function and categorization in the hypothesis-testing system in this type of task. \citet{Minda2015} tested some participants on rule-based (hypothesis-testing) and information-integration (associative) categories after they completed a task taxing executive function, while other participants completed an easy task before category learning. They found that the participants with depleted executive function resources showed difficulty learning rule-based categories but not information-integration categories. In addition, \citet{DeCaro2008} found that working memory capacity was positively related to category learning in the hypothesis-testing system but negatively related to category learning in the associative system. However, another study found working memory capacity to be similarly related to both associative and hypothesis-testing category learning \citep{Lewandowsky2012}. Inhibitory control has also been shown to be associated with optimal strategy use for both the associative and hypothesis-testing systems in older adults \citep{Maddox2010}. Thus, the fact that the current study does not find a specific relationship between use of the hypothesis-testing system and executive functioning is in line with the mixed nature of previous literature. However, the fact that no main effects of executive function were found in the Ashby perceptual category learning task is unexpected, as all of the studies cited above did find some relationship between category learning and executive function measures.  \par
	
\subsubsection{Sloutsky statistical density task}
	The Sloutsky statistical density task showed main effects of inhibition and task-switching on accuracy, and nonverbal IQ and planning on reaction time. Better inhibition was related to higher accuracy, while poorer task switching was related to better accuracy and poorer nonverbal IQ and less planning was related to faster reaction times. The main effect of inhibition on accuracy is somewhat different from a prior study showing a significant interaction between category sparsity and flanker performance, such that individuals with better inhibition skills also showed faster responses in a picture-word verification task specifically for sparse items \citep{Perry2016}. However, this study used multiple versions of the flanker task where the distractor arrows either appeared simultaneously with the target arrow, 150ms before the target arrow, or 500ms before. This allowed the authors to measure both the cost of incongruent distractors as well as the advantage of congruent distractors. The congruent advantage typically does not appear when the target and distractors are presented simultaneously, as they were in my study \citep{Botella2002}. It was this congruent advantage, and not the incongruent cost, that the authors found to be related to sparse categorization. Thus, my study may have been limited by its choice of flanker task; perhaps the interaction between system and inhibition would have been found with a congruent advantage effect rather than the classic flanker effect. Additionally, any results involving accuracy for this task should be interpreted cautiously, as the task had considerable ceiling effects. \par 
	The paper by \citet{Perry2016} also looked at how interfering with language resources affected performance on the picture-word verification task. They found that sparse categories were specifically affected by cathodal stimulation over Wernicke's area. That is, interfering with language resources led to slower and less accurate recognition of items from sparse categories, but had no effect on dense categories. Given these results, we might expect that individual differences in vocabulary or language measures like the CELF would be related to performance during hypothesis-testing blocks in the Sloutsky statistical density task. However, none of those effects were found for this task. Instead, the significant predictors were related to executive function and nonverbal IQ. It is hard to interpret the task-switching findings, as accuracy in this task was often close to ceiling. However, the planning and nonverbal IQ scores are somewhat puzzling; it is expected that individuals with better planning skills and higher nonverbal IQ would be faster at the category learning task, not slower. However, it is also possible that individuals who exhibited better planning and higher nonverbal IQ scores took more time and care in completing assessments, while those with poorer planning and lower nonverbal IQ scores were completing tasks quickly with less emphasis on accuracy. The Sloutsky statistical density task was quite easy for this population, which may explain why the proposed relationship was not seen in accuracy as well as reaction time. Thus, these effects may not reflect strong connections between the constructs of planning, nonverbal IQ, and category learning and may instead reflect an overall approach towards the experimental tasks and behavioral measures. \par
	
\subsubsection{Taxonomic/thematic task}
	Finally, few individual difference measures were related to performance on the taxonomic/thematic task. Specifically, I only found a main effect of nonverbal IQ on accuracy, such that participants with higher nonverbal IQ also showed better accuracy on the task overall. Some previous studies have found relations between executive function (specifically, cognitive flexibility) and taxonomic/thematic processing. In a few tasks, this was assessed by first reinforcing either taxonomic or thematic relations and then switching to the other type of relation. Studies show that younger school-age children are better at maintaining and switching to a thematic relation than a taxonomic relation, while older adults (ages 60-90) show a specific difficulty in switching to taxonomic relations but no difficulty in maintaining them \citep{Blaye2007, Maintenant2011}. This could suggest that taxonomic relations rely on executive function, such that individuals with poorer executive function (young children and older adults) have more difficulty switching to them. However, these studies do not directly measure individual differences in task switching, so this interpretation is purely speculation. Indeed, it is not born out by the current results, as no relationships between task switching and performance on the taxonomic/thematic task were found. \par 
	The lack of a relationship between vocabulary or grammar and the taxonomic/thematic task are somewhat unexpected. One study found that production of relational terms at 24 months was related to a thematic preference at 3 years of age \citep{Dunham1995}. Another study found that vocabulary was related to the strength of priming between thematically-related items but not for taxonomically-related items in school-aged children \citep{Brooks2014}. These studies suggest that vocabulary is somewhat related to thematic categorization in childhood. However, it is possible that this relationship disappears by adulthood, when semantic processing and vocabulary skills are fully developed. This would explain the lack of effects seen in the current study. \par
	
\subsubsection{Limitations and conclusions}	
	While the details of each study are discussed above, a striking pattern shown in this analysis is that different individual difference measures were related to performance on the three category learning tasks. Even those measures that showed relationships to more than one category learning task (e.g., nonverbal IQ) had different relationships with each task. Thus, not only was the core hypothesis not supported from this investigation, but the comparability of these three paradigms for studying category learning is not apparent. In other words, this analysis suggests that different skills underlie each of the three category learning tasks. To further address this issue, the next analysis will directly compare performance on the three category learning tasks without taking into account individual difference measures.
	 
	
\subsection{Results II: Cross-Paradigm Comparison}

For descriptive statistics on subject- and block-wise aggregated accuracy and reaction time in the category learning tasks, see Tables \ref{cross_comp_acc_desc} and \ref{cross_comp_rt_desc}.
\begin{table}[H]
\caption{Descriptive statistics for category learning tasks -- accuracy.}
\vspace{-10pt}
\begin{center}
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}p{4.5cm}YYYYYY}
\toprule
\multirow{2}{*}{Paradigm}    & \multicolumn{3}{c}{Associative} & \multicolumn{3}{c}{Hypothesis-testing} \\
                             & Mean    & SD      & Range       & Mean      & SD        & Range          \\
\midrule
Ashby perceptual             & 0.58    & 0.07    & 0.46-0.76   & 0.81      & 0.14      & 0.45-0.97      \\
Sloutsky statistical density & 0.95    & 0.09    & 0.6-1.00    & 0.92      & 0.15      & 0.2-1.0        \\
Taxonomic/thematic           & 0.84    & 0.13    & 0.21-1.00   & 0.84      & 0.16      & 0.21-1.00     \\
\bottomrule 
\label{cross_comp_acc_desc}
\end{tabularx}
\end{center}
\vspace{-10pt}
\small\textit{Note}. Only the 84 subjects included in the cross-paradigm analyses are summarized in this table.
\end{table}


\begin{table}[H]
\caption{Descriptive statistics for category learning tasks -- reaction time.}
\vspace{-10pt}
\begin{center}
\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}p{4.5cm}YYYYYY}
\toprule
\multirow{2}{*}{Paradigm}    & \multicolumn{3}{c}{Associative} & \multicolumn{3}{c}{Hypothesis-testing} \\
                             & Mean    & SD      & Range       & Mean      & SD        & Range          \\
\midrule
Ashby perceptual             & 738    & 250    & 122-1780 & 621      & 134      & 131-1030      \\
Sloutsky statistical density & 792    & 206    & 431-1320 & 667      & 151      & 413-1210        \\
Taxonomic/thematic           & 1730    & 752    & 786-6030 & 1920      & 545      & 918-3500     \\
\bottomrule 
\label{cross_comp_rt_desc}
\end{tabularx}
\end{center}
\vspace{-10pt}
\small\textit{Note}. Only the 84 subjects included in the cross-paradigm analyses are summarized in this table.
\end{table}

\subsubsection{Data Processing}

Concordant with an \textit{a priori} power analysis and pre-registration, only the first 84 undergraduate students with complete data were included in the analyses reported in this section.  \par
	\textbf{Ashby perceptual category learning task.} For this task, II blocks were labeled as associative and RB as hypothesis-testing. Accuracy and reaction time were measured for this task. Accuracy was summarized by subject and system. For reaction time, only accuracy trials were used. Outliers were removed on a by-trial basis using the same method described in the cross-paradigm analysis. Then, reaction time was summarized by subject and system. Next, we constructed boxplots to summarize mean RTs for each system and paradigm. Subjects who were clear outliers for both systems within a given paradigm were excluded (1 participant) and replaced with the next participant. Accuracy and reaction time were then Yeo-Johnson transformed to reduce skewness, as well as centered and scaled. At this point, any subjects with a z-score of less than -3 or greater than 3 were considered outliers and removed from further analysis. \par
	\textbf{Sloutsky statistical density task.} In this task, the unsupervised-dense block was considered to engage the associative system, and the supervised-sparse block was considered to engage the hypothesis-testing system. The other two blocks were discarded. Any participants who did not respond correctly to at least 6 of the 8 catch trials for a given block were removed from future analyses. Thus, all subjects reported in analyses using this task had at least 75\% accuracy on catch trials in both blocks. Accuracy was summarized by subject and system. Reaction time outliers were removed on a by-trial basis as described in the cross-paradigm analysis and reaction time was then summarized by subject and system. Next, we constructed boxplots to summarize mean RTs for each system and paradigm. No subjects for this task were clear outliers. Accuracy and reaction time were then transformed, centered, and scaled. At this point, any subjects with a z-score of less than -3 or greater than 3 were considered outliers and removed from further analysis. \par
	\textbf{Taxonomic/thematic task.} For this task, taxonomic blocks were associative and thematic blocks were hypothesis-testing. Practice trials were discarded before analysis. Accuracy was summarized by subject and system. Reaction time outliers were removed using the same method as above, and then reaction time was summarized by subject and system. Next, we constructed boxplots to summarize mean RTs for each system and paradigm. Subjects who were clear outliers for both systems within a given paradigm were excluded (2 participants) and replaced with the next  2 participants. Accuracy and reaction time were then transformed, centered, and scaled. At this point, any subjects with a z-score of less than -3 or greater than 3 were considered outliers and removed from further analysis. \par	
	
\begin{table}[H]
\caption{Correlations between category learning task measures -- accuracy.}
\vspace{-10pt}
\begin{center}
\begin{tabularx}{\textwidth}{p{6cm}YYYYYY}
\toprule
\multicolumn{1}{c}{Task}           & 1     & 2    & 3     & 4     & 5    & 6 \\
\midrule
1. HT Ashby perceptual             & -     &      &       &       &      &   \\
2. HT Sloutsky statistical density & 0.21  & -    &       &       &      &   \\
3. HT Taxonomic/thematic           & -0.07 & 0.18 & -     &       &      &   \\
4. AS Ashby perceptual             & 0.37* & 0.12 & 0.00  & -     &      &   \\
5. AS Sloutsky statistical density & 0.19  & 0.08 & 0.06  & 0.14  & -    &   \\
6. AS Taxonomic/thematic           & 0.05  & 0.14 & 0.39* & -0.12 & 0.23 & - \\
\bottomrule
\label{catlearn_corr_acc}
\end{tabularx}
\end{center}
\vspace{-10pt}
\small\textit{Note}. Only the 84 subjects included in the cross-paradigm analyses are included in this table. HT = hypothesis-testing, AS = associative. * \textit{p} \textless 0.05, Bonferroni corrected.
\end{table}		

\begin{table}[H]
\caption{Correlations between category learning task measures -- reaction time.}
\vspace{-10pt}
\begin{center}
\begin{tabularx}{\textwidth}{p{6cm}YYYYYY}
\toprule
\multicolumn{1}{c}{Task}           & 1     & 2    & 3     & 4     & 5    & 6 \\
\midrule
1. HT Ashby perceptual             & -     &       &       &       &      &   \\
2. HT Sloutsky statistical density & 0.30  & -     &       &       &      &   \\
3. HT Taxonomic/thematic           & 0.24  & 0.36* & -     &       &      &   \\
4. AS Ashby perceptual             & 0.58* & 0.25  & 0.24  & -     &      &   \\
5. AS Sloutsky statistical density & 0.29  & 0.49* & 0.26  & 0.31  & -    &   \\
6. AS Taxonomic/thematic           & 0.13  & 0.22  & 0.54* & 0.19  & 0.23 & - \\
\bottomrule
\label{catlearn_corr_acc}
\end{tabularx}
\end{center}
\vspace{-10pt}
\small\textit{Note}. Only the 84 subjects included in the cross-paradigm analyses are included in this table. HT = hypothesis-testing, AS = associative. * \textit{p} \textless 0.05, Bonferroni corrected.
\end{table}	
	
\subsubsection{Accuracy}
To investigate whether accuracy was comparable across paradigms, we constructed a mixed-effects model with random intercepts for subject. Adding the fixed effects of paradigm and system significantly improved model fit, $\chi^{2}(3)$ = 32.24, \textit{p} \textless 0.0001. Adding the interaction between paradigm and system further increased fit, $\chi^{2}(2)$ = 75.60, \textit{p} \textless 0.0001. Thus, the final model predicted the accuracy z-scores from paradigm, system, and their interaction. This model revealed a significant main effect of system, \textit{F}(1,415) = 38.37, \textit{p} \textless 0.0001, as well as a significant interaction between paradigm and system, \textit{F}(2,415) = 41.00, \textit{p} \textless 0.0001. To further investigate this interaction, we conducted three follow-up models each testing the effect of system within a given paradigm. \par
	The first model revealed a significant main effect of system in the perceptual category learning paradigm, \textit{F}(1,83) = 210.27, \textit{p} \textless 0.0001. A follow-up t-test confirmed that accuracy was significantly higher for the hypothesis-testing system, \textit{t}(133) = -11.90, \textit{p} \textless 0.0001. The second model revealed no main effect of system in the statistical density paradigm, \textit{F}(1,83) = 0.92, \textit{p}  = 0.34. A follow-up t-test confirmed this result, \textit{t}(135) = 0.91, \textit{p}  = 0.36.  Finally, the third model showed no main effect of system in the taxonomic-thematic paradigm, \textit{F}(1,83) = 0.73, \textit{p}  = 0.40. This was confirmed by a follow-up t-test, \textit{t}(164) = -0.66, \textit{p}  = 0.51. \par
	Overall these results suggest that these three paradigms are not comparable. While no differences between systems were found for the statistical density and taxonomic-thematic tasks, the perceptual category learning task showed a different pattern. However, the statistical density task may have been suffering from ceiling effects. Of the 84 total subjects, we saw average accuracy values of 0.9 or higher during the statistical density paradigm in 76 subjects for the associative block and 58 subjects for the hypothesis-testing block. Thus, the statistical density task may not be sufficiently difficult to detect differences between systems in accuracy.
 \par
\subsubsection{Reaction Time}
To investigate whether reaction time was comparable across paradigms, we constructed a mixed-effects model with random intercepts for subject. Adding the fixed effects of paradigm and system significantly improved model fit,  $\chi^{2}(3)$ = 13.48, p = 0.003. Adding the interaction between paradigm and system further increased fit,  $\chi^{2}(2)$  = 44.65, \textit{p} \textless 0.0001. Thus, the final model predicted the accuracy z-scores from paradigm, system as well as their interaction. This model revealed a significant main effect of system, \textit{F}(1,415) = 14.62, \textit{p} = 0.0001, but no main effect of paradigm, \textit{F}(2,415) = 0.017, \textit{p} = 0.84. The interaction between system and paradigm was also significant, \textit{F}(2,415) = 23.30, \textit{p} \textless 0.0001. To further investigate this interaction, we conducted three follow-up models each testing the effect of system within a given paradigm. \par
	The first model revealed a significant main effect of system in the perceptual category learning paradigm, \textit{F}(1,83) = 29.96, \textit{p} \textless 0.0001. A follow-up t-test confirmed that reaction time was significantly faster for the hypothesis-testing system, \textit{t}(141) = 3.73, \textit{p} = 0.0002. The second model revealed a significant main effect of system in the statistical density paradigm, \textit{F}(1,83) = 44.00, \textit{p} \textless 0.0001. A follow-up t-test showed that again reaction time was faster for the hypothesis-testing system, \textit{t}(165) = 4.62, \textit{p} \textless 0.0001.  Finally, the third model also showed a main effect of system in the taxonomic-thematic paradigm, \textit{F}(1,83) = 14.96, \textit{p} = 0.0002. However, for this paradigm the pattern was flipped. Reaction times were faster for the associative system, \textit{t}(162) = -2.68, \textit{p} = 0.008.
	
\subsection{Discussion II: Cross-Paradigm Comparison}

This analysis aimed to directly compare three dual-systems approaches to category learning. From a theoretical standpoint, considerable similarities can be drawn between these approaches. They each consider two category structure types, which can be mapped onto similarity- and rule-based categories. In addition, two of the approaches posit very similar systems for learning these categories, each specifically adapted to one type of category. One system best learns similarity-based categories by integrating and compressing multiple features using an iterative and associative process. The other system uses higher-order skills like working memory and executive functions to select and test hypotheses about category-relevant features. This system is best for learning rule-based categories. Each approach uses a different paradigm to measure how individuals learn these different category structures. I hypothesized that while there are considerable task-related differences among the paradigms, each paradigm would engage the relevant category learning system in a given block. Thus, I expected to see a main effect of system but no effect of paradigm, indicating that each task separately engaged the two systems in different blocks. \par
	I did not find these hypothesized results in either accuracy or reaction time. In accuracy, two of the paradigms showed no difference while one showed a different pattern. For perceptual category learning, accuracy was much lower for the associative system than for the hypothesis-testing system. In fact, mean accuracy in the associative block of the perceptual task was barely above chance, indicating that participants showed little learning of these categories. In contrast, no accuracy differences were seen between the two blocks in the taxonomic/thematic and statistical density task. The statistical density paradigm also suffered from considerable ceiling effects. In reaction time, we again saw paradigm-related differences. In both the perceptual task and the statistical density task, participants were significantly faster in the hypothesis-testing block than in the associative block. However, this pattern was reversed for the taxonomic/thematic task. \par
	
\subsubsection{Task differences}

	
\subsubsection{Limitations and conclusions}

A key takeaway from this study is that despite the theoretical similarities behind these approaches, the tasks they use to measure category learning are not directly comparable. Even after accounting for scaling considerations by using \textit{z}-scores, the relationship between category learning in the two systems is not consistent across paradigms. This could simply be explained by task and stimuli differences; perhaps each approach is indeed trying to measure the same types of processing, but they are taxing the systems differently. Another possibility is that each approach is fundamentally trying to explain different, albeit related, phenomena. We will consider both of these possibilities in the general discussion. 

\end{document}

