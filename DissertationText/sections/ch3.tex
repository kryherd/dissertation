\documentclass[../dissertation.tex]{subfiles}
 
\begin{document}

\section{Experiment 2}

	This experiment tests the core hypothesis of this dissertation. Namely, I use a within-subjects design to test whether executive function is specifically related to categorization in the hypothesis-testing system while verbal labels are specifically related to associative system categorization. This experiment also uses three different category learning tasks, allowing me compare different category learning paradigms and to test the core hypothesis for multiple approaches. \par
	In this experiment, I use vocabulary as a proxy for labeling. Vocabulary measures should reflect the link between a word and its meaning. Further, this link should be more elaborated than those built in a paired-associate learning task. Thus, individual differences in vocabulary should give some insight into how well participants use labels in learning categories. For executive function, I selected three different measures. Multiple studies have shown that while executive function is sometimes talked about as a single construct, it actually is made up of separable components \citep{Miyake2000, Karr2018}. The components I chose to focus on were inhibition, switching, and planning. \par
	I chose to compare the COVIS, statistical density, and taxonomic-thematic approaches to category learning. These three approaches represent a broad spectrum of category learning. COVIS exclusively focuses on perceptual categories, while the statistical density approach uses constructed stimuli that can be mapped onto real-world objects at least somewhat. Finally, the taxonomic-thematic approach is almost always applied to real-world objects with which participants have experience. Since these three approaches are so different in the types of categories they try to explain, results showing similarities between them would be strong evidence for an overarching dual-systems framework. I decided to use common paradigms from each approach as a starting point for comparison. Given the theoretical similarities between the approaches, I hypothesize that task differences will not be sufficient to affect the relationship between the two systems differently for each approach.

\subsection{Method}
\subsubsection{Participants}
XX participants were recruited from the psychology undergraduate participant pool at the University of Connecticut (X Female, X Male, mean age = X).
\subsubsection{Category Learning Tasks}
This experiment used three different category learning tasks, each based on a different approach to category learning. We used these three tasks to investigate whether the paradigms used in different approaches engage category learning systems in a similar way. The order of category learning tasks was counterbalanced across participants. All category learning tasks were presented using PsychoPy v.1.84.2 \citep{Peirce2007}. \par 
\textbf{Sloustky statistical density task.} This task used the same procedure and stimuli as the task described in Experiment 1. However, instead of completing only two blocks, participants completed all four blocks. Because the previous experiment showed few significant order effects, the order of the four blocks was randomly generated for each participant. \par
\textbf{Ashby perceptual category learning task.} There were two versions to this task: Information-Integration (II) and Rule-Based (RB). Participants completed the II version and then the RB version. Prior research has shown that when participants are asked to switch between the declarative (hypothesis-testing) and implicit (associative) systems, they end up using rule-based strategies from the declarative system for all trials. Thus, by engaging the implicit system first, we aimed to reduce transfer effects between versions as much as possible. \par
In each version of the task, participants were told that they would be learning two categories and that perfect performance was possible. They were also told to be as quick and accurate as possible. In each trial, participants viewed a Gabor patch that belonged to one of the two categories. Each patch subtended 11\degree of visual angle. The stimuli were generated using category parameters from \citet{Maddox2003}. The participant then had 5000ms to press a key, indicating which category they believed the stimulus belonged to. After a response, the participant received feedback ("Correct" or "Incorrect"). Feedback was presented for 1000ms, and then the next trial began. If the participant took more than 5000ms to respond, they saw "Too Slow" and proceeded to the next trial. Participants completed five runs of each version. Each run had 80 trials (40 from each category) presented in a random order. Thus, in total participants completed 400 II trials and 400 RB trials. \par
\textbf{Taxonomic/thematic task.} This task was adapted from \citet{Murphy2001} and \citet{Kalenine2009}. There were also two versions of this task: one taxonomic and one thematic. Version order was counterbalanced across subjects, with some participants getting the taxonomic version first and others the thematic version first. Most versions of this type of task allow participants to choose the item that is most "semantically related," and thus do not ask participants to make either taxonomic or thematic choices on any given trial. As such, little research has looked at switching between taxonomic and thematic semantic judgments. Thus, counterbalancing was applied to control for order effects. \par
The stimuli were images taken from \citet{Konkle2010}. We chose to use images in order to avoid automatic language processing. While participants likely did engage linguistic resources during the task, this should be due to how language relates to categorization rather than the features of the stimuli themselves. In each trial, four images were presented: a target, a taxonomically-related item, a thematically-related item, and an unrelated item. Taxonomically- and thematically-related items were chosen based on norms from \citet{Landrigan2016} where available. The \citet{Landrigan2016} norms were based on word stimuli rather than the images available from \citet{Konkle2010}; as such, not all of the available images were normed. For images without norming information, we used our best judgment to pick items for each type of relation. \par
For each version, participants were told that they would be categorizing objects. They were told to pick the option that "goes best with" (thematic) or is "most similar to" (taxonomic) the target item. We chose these instructions based on previous research showing that slight differences in task instructions affect taxonomic and thematic judgments \citep{Lin2001}. After instructions, participants got five practice trials. In each trial, the images were shown for 5000ms and participants had unlimited time to make a response. The practice trials were identical for the taxonomic and thematic versions of the task. After each response, participants received feedback ("Correct!" or "Oops!") for 1000ms. Once the practice trials were completed, participants received 24 test trials. While some images were seen in multiple trials, the 4-image combination for each trial was unique across the taxonomic and thematic versions of the task.
\subsubsection{Executive Function Tasks} 
To measure executive function, we used three different tasks taken from the Psychology Experiment Building Language (PEBL) test battery \citep{Mueller2014}. We chose three tasks to try and tap multiple aspects of executive function, including inhibition, planning, and task-switching. All three tasks were presented using the PEBL software.\par
\textbf{Flanker task (inhibition).} This task was an implementation of the \citet{Eriksen1979} flanker task, using a method similar to \citet{Stins2007}. In each trial, participants viewed a set of five arrows and were asked to respond based on the direction in which the center arrow was pointing (left or right). In congruent trials, all arrows faced the same way. In incongruent trials, the four distractor arrows pointed in the opposite direction of the target (center) arrow. In neutral trials, the four distractor arrows were just horizontal lines without arrowheads. Participants completed 20 trials for each condition in a 2 (direction; left vs. right) x 3 (condition; congruent vs. incongruent vs. neutral) design, for a total of 120 trials. \textbf{how many empty trials??} Each trial began with a 500 ms fixation, followed by the stimulus which appeared for 800ms. Participants were only allowed to respond during the 800ms that the stimulus was on the screen. After a response, there was an inter-trial interval of 1000ms. Participants received 12 practice trials before the actual experiment to get used to the timing of each trial. During practice trials, each response was followed by feedback ("Correct", "Incorrect") as well as a number indicating RT for that trial. This feedback was not provided for the test trials. \par
\textbf{Switcher task (task-switching).} This task was taken from \citet{Anderson2012}. In this task, participants are presented with an array of colored shapes. Each colored shape has a single letter inside. For each trial, a single shape was indicated to be the target shape. Based on instructions at the top of the screen, participants were told to select a shape that matched the target shape on one of three dimensions (color, shape, or letter). Research from \citet{Miyake2004} has shown that cueing a dimension using its entire name (e.g. "shape") does not require as many language resources as cueing a dimension using a single letter (e.g., "s"). Since one of the core hypotheses of this study was that language supports executive functions in the hypothesis-testing system, we used a version of the switcher task that cued dimension using just a single letter. We expect that this version of the task requires individuals to represent dimensions/selection rules internally, similar to how they might represent possible category rules when learning rule-based categories. \par
The task consisted of nine different arrays of ten shapes. For each array, participants made ten responses. In the first three arrays, participants switched between two of the three dimensions in a fixed order (e.g., C - S - C - S, etc.). The relevant dimensions were different for each array. For the second three arrays, participants switched between all three dimensions still in a fixed order (e.g., S - C - L - S - C - L, etc). The specific order was different for each array. Finally, in the last three arrays participants switched between all three dimensions in a random order. Unlike previous arrays, in the last three participants were unable to anticipate the upcoming relevant dimension. \par
\textbf{Tower of London task (planning)}. This task was a computerized version of the one described in \citet{Shallice1982}. In this task, participants were shown a setup of colored disks in three stacks as well as a target setup. They were given a limited number of moves to make their setup match the target setup. Participants could only have one disk in their "hand" at a time, and they could only pick the top disk up off of any stack. The trials varied in the number of steps required to match the target setup from 2 to 5, with easier (2 step) trials at the beginning of the task and harder (5 step) trials at the end of the task. Participants were encouraged to take their time and plan out their moves before beginning each trial.

\subsubsection{Behavioral Measures} 
Finally, we used four different behavioral assessments to measure vocabulary, syntax, and nonverbal IQ..
\textbf{Nelson-Denny vocabulary subtest.} To measure vocabulary, we used the same Nelson-Denny vocabulary subtest described in experiment 1. \par
\textbf{Clinical Evaluation of Language Fundamentals recalling sentences and formulated sentences subtests}. We used the CELF here to measure individuals differences in syntax production and perception. The recalling sentences subtest allowed us to look at receptive grammar, while the formulated sentences subtest provided a measure of expressive grammar. In the formulated subtest, participants view a scene and are asked to make a sentence containing a target word about that scene. Often, the target word encourages certain syntactic structures (e.g., "because"). \par
\textbf{Raven's Advanced Matrices.} We used Raven's Advanced matrices to measure nonverbal IQ, as described in Experiment 1.

\subsection{Procedure}

Each participant completed all of the category learning and executive function tasks, as well as all of the behavioral measures. CELF responses were audio-recorded to allow for offline scoring. To allow multiple subjects to be run in a single timeslot, some participants received tasks in a shuffled order. All together, the tasks and behavioral measures took about an hour and a half.

\subsection{Results I: Cross-Paradigm Comparison}

Descriptive statistics for accuracy and reaction time in all category learning tasks can be found in Table TABLE HERE.

\subsubsection{Data Processing}

Concordant with an \textit{a priori} power analysis and pre-registration, only the first 84 undergraduate students with complete data were included in the analyses reported in this section.  \par
	\textbf{Ashby perceptual category learning task.} For this task, II blocks were labeled as associative and RB as hypothesis-testing. Accuracy and reaction time were measured for this task. Accuracy was summarized by subject and system. For reaction time, only accuracy trials were used. Outliers were removed on a by-trial basis using the same method described in the cross-paradigm analysis. Then, reaction time was summarized by subject and system. Next, we constructed boxplots to summarize mean RTs for each system and paradigm. Subjects who were clear outliers for both systems within a given paradigm were excluded (1 participant) and replaced with the next participant. Accuracy and reaction time were then Yeo-Johnson transformed to reduce skewness, as well as centered and scaled. At this point, any subjects with a z-score of less than -3 or greater than 3 were considered outliers and removed from further analysis. \par
	\textbf{Sloutsky statistical density task.} In this task, the unsupervised-dense block was considered to engage the associative system, and the supervised-sparse block was considered to engage the hypothesis-testing system. The other two blocks were discarded. Any participants who did not respond correctly to at least 6 of the 8 catch trials for a given block were removed from future analyses. Thus, all subjects reported in analyses using this task had at least 75\% accuracy on catch trials in both blocks. Accuracy was summarized by subject and system. Reaction time outliers were removed on a by-trial basis as described in the cross-paradigm analysis and reaction time was then summarized by subject and system. Next, we constructed boxplots to summarize mean RTs for each system and paradigm. No subjects for this task were clear outliers. Accuracy and reaction time were then transformed, centered, and scaled. At this point, any subjects with a z-score of less than -3 or greater than 3 were considered outliers and removed from further analysis. \par
	\textbf{Taxonomic/thematic task.} For this task, taxonomic blocks were associative and thematic blocks were hypothesis-testing. Practice trials were discarded before analysis. Accuracy was summarized by subject and system. Reaction time outliers were removed using the same method as above, and then reaction time was summarized by subject and system. Next, we constructed boxplots to summarize mean RTs for each system and paradigm. Subjects who were clear outliers for both systems within a given paradigm were excluded (2 participants) and replaced with the next  2 participants. Accuracy and reaction time were then transformed, centered, and scaled. At this point, any subjects with a z-score of less than -3 or greater than 3 were considered outliers and removed from further analysis. \par
\subsubsection{Accuracy}
To investigate whether accuracy was comparable across paradigms, we constructed a mixed-effects model with random intercepts for subject. Adding the fixed effects of paradigm and system significantly improved model fit, $\chi^{2}(3)$ = 32.24, \textit{p} \textless 0.0001. Adding the interaction between paradigm and system further increased fit, $\chi^{2}(2)$ = 75.60, \textit{p} \textless 0.0001. Thus, the final model predicted the accuracy z-scores from paradigm, system, and their interaction. This model revealed a significant main effect of system, \textit{F}(1,415) = 38.37, \textit{p} \textless 0.0001, as well as a significant interaction between paradigm and system, \textit{F}(2,415) = 41.00, \textit{p} \textless 0.0001. To further investigate this interaction, we conducted three follow-up models each testing the effect of system within a given paradigm. \par
	The first model revealed a significant main effect of system in the perceptual category learning paradigm, \textit{F}(1,83) = 210.27, \textit{p} \textless 0.0001. A follow-up t-test confirmed that accuracy was significantly higher for the hypothesis-testing system, \textit{t}(133) = -11.90, \textit{p} \textless 0.0001. The second model revealed no main effect of system in the statistical density paradigm, \textit{F}(1,83) = 0.92, \textit{p}  = 0.34. A follow-up t-test confirmed this result, \textit{t}(135) = 0.91, \textit{p}  = 0.36.  Finally, the third model showed no main effect of system in the taxonomic-thematic paradigm, \textit{F}(1,83) = 0.73, \textit{p}  = 0.40. This was confirmed by a follow-up t-test, \textit{t}(164) = -0.66, \textit{p}  = 0.51. \par
	Overall these results suggest that these three paradigms are not comparable. While no differences between systems were found for the statistical density and taxonomic-thematic tasks, the perceptual category learning task showed a different pattern. However, the statistical density task may have been suffering from ceiling effects. Of the 84 total subjects, we saw average accuracy values of 0.9 or higher during the statistical density paradigm in 76 subjects for the associative block and 58 subjects for the hypothesis-testing block. Thus, the statistical density task may not be sufficiently difficult to detect differences between systems in accuracy.
 \par
\subsubsection{Reaction Time}
To investigate whether reaction time was comparable across paradigms, we constructed a mixed-effects model with random intercepts for subject. Adding the fixed effects of paradigm and system significantly improved model fit,  $\chi^{2}(3)$ = 13.48, p = 0.003. Adding the interaction between paradigm and system further increased fit,  $\chi^{2}(2)$  = 44.65, \textit{p} \textless 0.0001. Thus, the final model predicted the accuracy z-scores from paradigm, system as well as their interaction. This model revealed a significant main effect of system, \textit{F}(1,415) = 14.62, \textit{p} = 0.0001, but no main effect of paradigm, \textit{F}(2,415) = 0.017, \textit{p} = 0.84. The interaction between system and paradigm was also significant, \textit{F}(2,415) = 23.30, \textit{p} \textless 0.0001. To further investigate this interaction, we conducted three follow-up models each testing the effect of system within a given paradigm. \par
	The first model revealed a significant main effect of system in the perceptual category learning paradigm, \textit{F}(1,83) = 29.96, \textit{p} \textless 0.0001. A follow-up t-test confirmed that reaction time was significantly faster for the hypothesis-testing system, \textit{t}(141) = 3.73, \textit{p} = 0.0002. The second model revealed a significant main effect of system in the statistical density paradigm, \textit{F}(1,83) = 44.00, \textit{p} \textless 0.0001. A follow-up t-test showed that again reaction time was faster for the hypothesis-testing system, \textit{t}(165) = 4.62, \textit{p} \textless 0.0001.  Finally, the third model also showed a main effect of system in the taxonomic-thematic paradigm, \textit{F}(1,83) = 14.96, \textit{p} = 0.0002. However, for this paradigm the pattern was flipped. Reaction times were faster for the associative system, \textit{t}(162) = -2.68, \textit{p} = 0.008.
	
\subsection{Discussion I: Cross-Paradigm Comparison}

This analysis aimed to directly compare three dual-systems approaches to category learning. From a theoretical standpoint, considerable similarities can be drawn between these approaches. They each consider two category structure types, which can be mapped onto similarity- and rule-based categories. In addition, two of the approaches posit very similar systems for learning these categories, each specifically adapted to one type of category. One system best learns similarity-based categories by integrating and compressing multiple features using an iterative and associative process. The other system uses higher-order skills like working memory and executive functions to select and test hypotheses about category-relevant features. This system is best for learning rule-based categories. Each approach uses a different paradigm to measure how individuals learn these different category structures. I hypothesized that while there are considerable task-related differences among the paradigms, each paradigm would engage the relevant category learning system in a given block. Thus, I expected to see a main effect of system but no effect of paradigm, indicating that each task separately engaged the two systems in different blocks. \par
	I did not find these hypothesized results in either accuracy or reaction time. In accuracy, two of the paradigms showed no difference while one showed a different pattern. For perceptual category learning, accuracy was much lower for the associative system than for the hypothesis-testing system. In fact, mean accuracy in the associative block of the perceptual task was barely above chance, indicating that participants showed little learning of these categories. In contrast, no accuracy differences were seen between the two blocks in the taxonomic/thematic and statistical density task. The statistical density paradigm also suffered from considerable ceiling effects. In reaction time, we again saw paradigm-related differences. In both the perceptual task and the statistical density task, participants were significantly faster in the hypothesis-testing block than in the associative block. However, this pattern was reversed for the taxonomic/thematic task. \par
	A key takeaway from this study is that despite the theoretical similarities behind these approaches, the tasks they use to measure category learning are not directly comparable. Even after accounting for scaling considerations by using \textit{z}-scores, the relationship between category learning in the two systems is not consistent across paradigms. This could simply be explained by task and stimuli differences; perhaps each approach is indeed trying to measure the same types of processing, but they are taxing the systems differently. Another possibility is that each approach is fundamentally trying to explain different, albeit related, phenomena. We will consider both of these possibilities in the general discussion. 

\subsection{Results II: Individual Differences}

Descriptive statistics for all individual difference measures can be found in Table TABLE HERE.

\subsubsection{Data Processing}
	An \textit{a priori} power analysis showed that 132 subjects would be needed for this experiment. As is noted above, more than 132 subjects participated in the experiment. Due to the many assessments being collected, issues with the experimental paradigm, and experimental time constraints, there was a considerable amount of data missingness. Thus, each analysis discussed below uses the first 132 subjects with full data for all measures included in that analysis. \par
	\textbf{Ashby perceptual category learning task.} For this task, II blocks were labeled as associative and RB as hypothesis-testing. Accuracy and reaction time were measured for this task. Accuracy was summarized by subject and system. For reaction time, only accuracy trials were used. Outliers were removed on a by-trial basis using the same method described in the cross-paradigm analysis. Then, reaction time was summarized by subject and system. A single subject had reaction times 8 SD higher than the mean; this subject's data was removed and replaced with another subject. Accuracy and reaction time were then Yeo-Johnson transformed to reduce skewness, as well as centered and scaled. \par
	\textbf{Sloutsky statistical density task.} In this task, the unsupervised-dense block was considered to engage the associative system, and the supervised-sparse block was considered to engage the hypothesis-testing system. The other two blocks were discarded. Again, participants were removed for failure to reach criterion on catch trials (as described above). Accuracy was summarized by subject and system. Reaction time outliers were removed on a by-trial basis as described in the cross-paradigm analysis and reaction time was then summarized by subject and system. Accuracy and reaction time were then transformed, centered, and scaled. \par
	\textbf{Taxonomic/thematic task.} For this task, taxonomic blocks were associative and thematic blocks were hypothesis-testing. Practice trials were discarded before analysis. Accuracy was summarized by subject and system. Reaction time outliers were removed using the same method as above, and then reaction time was summarized by subject and system. Accuracy and reaction time were then transformed, centered, and scaled. \par
	\textbf{Flanker task.} The flanker effect was calculated by first selecting only incongruent or congruent trials. Then, the average reaction time was calculated for each subject for both trial types. Finally, the average reaction time for congruent trials was subtracted from the average reaction time for incongruent trials for each subject. This measure was then centered and scaled but not transformed. \par
	\textbf{Switcher task.} The switcher effect was calculated by  selecting the 3-dimension ordered and 3-dimension random blocks. Average reaction time was calculated for each subject in each of these blocks. Then, the reaction time for 3-dimension ordered was subtracted from the reaction time for 3-dimension random. This measure was then transformed, centered, and scaled. \par 
	\textbf{Tower of London task.} The main metric calculated for this task was the ratio of planning time to total trial time for each subject. Planning time and total trial time were summarized for each subject. Next, planning time was divided by total trial time. This measure was then transformed, centered, and scaled. \par 
	\textbf{Nelson-Denny, CELF, RAM}. Nelson-Denny and CELF were both standardized using their respective norms. Raven's was not standardized. All 4 measures (Nelson-Denny vocabulary, CELF Recalling Sentences, CELF Formulated Sentences, and Raven's Advanced Matrices) were all transformed, centered, and scaled.
\subsubsection{Accuracy}

\textbf{Ashby perceptual category learning task.} \par 
\textbf{Sloutsky statistical density task.}  To investigate how the individual difference measures related to task performance, I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system marginally improved model fit,  $\chi^{2}(1)$ = 3.38, p = 0.07. In the next step, I added RAM, vocabulary, and the three executive function tasks as fixed effects. This step significantly improved fit, $\chi^{2}(5)$  = 20.24, \textit{p} = 0.001. However, adding the interactions between system and the individual difference measures (except RAM) did not further improve fit,  $\chi^{2}(4)$ = 3.03, p = 0.55. Thus, the final model predicted accuracy in this task from system and the individual difference measures, but not their interactions. This model revealed a significant main effect of flanker, \textit{F}(1,126) = 4.93, \textit{p} = 0.03, and a significant main effect of switcher, \textit{F}(1,126) = 9.70, \textit{p} = 0.003. The coefficient associated with flanker was positive (\textit{b} = 0.16, \textit{SE} = 0.07), while the coefficient associated with switcher was negative, (\textit{b} = -0.21, \textit{SE} = 0.07). There were also marginally significant effects of system, \textit{F}(1,126) = 3.40, \textit{p} = 0.07, Tower of London, \textit{F}(1,126) = 3.56, \textit{p} = 0.06, and vocabulary, \textit{F}(1,126) = 2.82, \textit{p} = 0.10. Thus, accuracy on the Sloutsky statistical density task is positively related to flanker performance and negatively related to switcher performance. However, as in the prior analysis, these results should be interpreted with caution, as large ceiling effects were found in accuracy for this task. \par
\textbf{Taxonomic/thematic task.}  Again I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system significantly improved model fit,  $\chi^{2}(1)$ = 8.59, p = 0.003. Adding the individual difference measures also significantly improved fit, $\chi^{2}(5)$  = 11.46, \textit{p} = 0.04. However, adding the interactions between system and the individual difference measures (except RAM) did not further improve fit,  $\chi^{2}(4)$ = 1.81, p = 0.77. Thus, the final model predicted accuracy in this task from system and the individual difference measures, but not their interactions. This model revealed a significant main effect of system, \textit{F}(1,131) = 8.80, \textit{p} = 0.003, and a significant main effect of RAM, \textit{F}(1,126) = 4.78, \textit{p} = 0.031. The coefficient associated with RAM was positive (\textit{b} = 0.17, \textit{SE} = 0.08), while the coefficient associated with switcher was negative. Thus, accuracy on the taxonomic/thematic task is positively related to performance on RAM.

\subsubsection{Reaction Time}

\textbf{Ashby perceptual category learning task.} \par 
\textbf{Sloutsky statistical density task.}  Once again I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system significantly improved model fit,  $\chi^{2}(1)$ = 53.01, p \textless 0.001. Adding the individual difference measures as fixed effects also significantly improved fit, $\chi^{2}(5)$  = 17.62, \textit{p} = 0.003. However, adding the interactions between system and the individual difference measures (except RAM) only marginally improved fit,  $\chi^{2}(4)$ = 8.13, p = 0.09. Thus, the final model predicted accuracy in this task from system and the individual difference measures, but not their interactions. This model revealed a significant main effect of system, \textit{F}(1,131) = 64.75, \textit{p} \textless 0.001, a main effect of RAM, \textit{F}(1,126) = 4.26, \textit{p} = 0.04, and a significant main effect of Tower of London, \textit{F}(1,126) = 9.18, \textit{p} = 0.003. The coefficient associated with RAM was positive (\textit{b} = 0.16, \textit{SE} = 0.08), as was the coefficient associated with Tower of London, (\textit{b} = 0.22, \textit{SE} = 0.07). Thus, participants who took longer to respond in this task also performed better on RAM and spent a larger portion of their total time planning in Tower of London. \par
\textbf{Taxonomic/thematic task.}  I constructed a mixed-effects model with random intercepts for subject. Adding the fixed effect of system significantly improved model fit,  $\chi^{2}(1)$ = 19.70, p \textless 0.001. However, adding the individual difference measures did not further improve fit,  $\chi^{2}(4)$ = 3.89, p = 0.42. Indeed, the only effect even close to significant in this model was a marginal main effect of Tower of London, \textit{F}(1,127) = 3.17, \textit{p} = 0.08. Thus, reaction time on the taxonomic/thematic task is not related to any of the individual difference measures.

\subsection{Discussion II: Individual Differences}

\end{document}

