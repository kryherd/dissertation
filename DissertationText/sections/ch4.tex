\documentclass[../dissertation.tex]{subfiles}
 
\begin{document}

\section{General Discussion}

\subsection{Language in a dual-systems model: summary of results}
\subsubsection{Language and the interaction between two systems}
In Experiment 1, I tested whether the order in which an individual engages different category learning systems affects learning in those systems. I also tested whether any observed order effects vary according to general language ability. I predicted that switching from the hypothesis-testing system to the associative system would produce a cost, while switching from associative to hypothesis-testing would not. This prediction was based on prior research showing dominance of the hypothesis-testing system \citep{Erickson2008, Ashby2010}. I also predicted that switch costs would be higher in individuals with poorer language ability, since my prior research showed that low-language individuals had difficulty switching away from suboptimal learning strategies \citep{Ryherd2019}. \par 
Neither of these hypotheses were supported by the data. Instead of poorer performance in second blocks, which would indicate a switch cost, all order effects were driven by better performance in second blocks, a result more consistent with a learning effect. In addition, there was no interaction between language ability and order in any of the three analyses. Thus, the observed learning effect did not depend on an individual's language ability.

\subsubsection{Individual differences and dual-systems category learning}

	In the first analysis of Experiment 2, I investigated whether individual differences in vocabulary and executive function would predict performance on three category learning tasks. I expected to see that vocabulary, acting as a measure of labeling ability, would be related to performance on associative category learning blocks, while executive function would be related to performance on hypothesis-testing blocks. I expected these relationships to hold across all three category learning tasks. \par
	The results did not support these hypotheses. First, performance in each category learning task was related to different individual difference measures. Vocabulary showed significant effects in the Ashby category learning task, while planning affected performance in the Sloutsky statistical density and taxonomic/thematic tasks. Further, the relationship between planning and performance differed for the Sloutsky and taxonomic/thematic tasks. \par 
	In addition, most of the relationships I found did not match the overall predictions. While I expected to see a positive relationship between vocabulary and associative learning and no relationship for hypothesis-testing learning, it seemed to facilitate hypothesis-testing learning and impair associative learning in the Ashby perceptual category task. Further, higher planning skill was specifically related to poorer performance in the associative block of the Sloutsky task. The only predicted relationship was in the taxonomic/thematic task, where planning (an executive function measure) was positively related to performance in the hypothesis-testing but not the associative block.
	
\subsubsection{Levels of processing and the dual-systems model}

The second analysis in Experiment 2 was the first to directly compare performance across three different paradigms designed to test dual-systems approaches to category learning. These approaches have considerable theoretical similarities (see Chapter \ref{intro}); as such, it is conceivable that the experimental tasks used to test these approaches measure the same type of processing. I tested subjects on the three tasks and hypothesized that the pattern of performance would be similar across all three. That is, if participants showed higher accuracy on the associative block than the hypothesis-testing block of one task, they should show the same pattern for the other two tasks. \par
This hypothesis was not supported by the results. In accuracy, participants showed significant differences between the associative and hypothesis-testing blocks for the Ashby perceptual category learning task, but no block differences for the Sloutsky statistical density task or the taxonomic/thematic task. In reaction time, the Ashby and Sloutsky tasks showed similar patterns, while the taxonomic/thematic task was the reverse. Together, the results suggest that these three tasks as they are typically administered are not comparable and do not engage the same processing.


\subsection{Rethinking the dual-systems model of categorization}

	In the introduction to this document, I drew parallels across six different approaches to category learning. Each approach split category learning and categorization in two. Some categories were probabilistic, with fuzzy boundaries for inclusion. Others were based on clearly-defined rules. Many of these approaches proposed a dual-systems model for learning these two types of categories, positing a different system specifically tuned for each category type's structure. I combined these different approaches to suggest a more unified dual-systems theory of category learning. However, the experiments described above do not support this combined theory. Rather than showing similarities across the different approaches, these experiments revealed different patterns of performance for each task as well as different relationships with individual difference measures. Thus, the theoretical similarities seen across these approaches are not apparent in the data. This casts doubt on a single unified dual-systems theory of category learning. Indeed, this investigation is not the first to question dual-systems models.

\subsubsection{Existing critiques of dual-systems models}

COVIS is the main dual-systems model some researchers have argued against, which is likely due to its prominence and popularity. While many studies have shown dissociations between the two category learning systems proposed in COVIS, some more recent studies have made strong claims against the existing evidence. For example, one paper points out that common stimuli used in a COVIS paradigm are not sufficiently matched. Thus, the double dissociations seen in these paradigms may be due to stimulus characteristics rather than differential processing. For example, one study tested the effect of feedback using stimuli that were matched on participant error rates, category separation, and relevant dimensions \citep{Edmunds2015}. This study did not find a difference between category type, a finding in opposition to previous studies with less carefully-matched stimuli \citep{Ashby2002, Maddox2003}. \par 
	Another critique of the COVIS framework is its assumption that items learned by the associative system are learned nonverbally and implicitly, and thus are not available to the conscious mind. To test this assumption, another study tested recognition memory for exemplars of rule-based (hypothesis-testing) and information-integration (associative) stimuli after the categories were learned \citep{Edmunds2016}. Recognition memory is commonly assumed to test explicit memory \citep{Gabrieli1995}. If participants could reliably recognize exemplars from information-integration categories, it would be unlikely that these items were being learned truly implicitly. In fact, \citet{Edmunds2016} found that participants not only were able to recognize information-integration (associative) stimuli at an above-chance rate, they were also more accurate at recognizing information-integration (associative) stimuli than rule-based (hypothesis-testing) stimuli. This suggests that instances that should have been learned using the associative system implicitly were at least available to explicit memory after learning. Further support for this critique comes from a study showing that participants produced verbal reports of their learning strategies that matched model-based strategy determination \cite{Edmunds2015}. Thus, participants were able to access both the items they had learned as well as the method they had learned for categorization.  \par 
	A third critique of COVIS centers on the previously-mentioned mathematical models, which are used to verify whether an individual is using associative or hypothesis-testing strategies to learn categories. Decision-bound strategy analysis fits different decision boundary models to the category responses made by participants to determine their learning strategies \citep{Maddox1993}. However, a recent study used simulations to test the validity of this type of analysis \citep{Edmunds2018}. The authors created simulated participants who used either associative or hypothesis-testing strategies, and then ran decision-bound strategy analysis on the simulated data. They found that over a third of simulated participants using a hypothesis-testing strategy were misidentified, while almost all simulated participants using an associative strategies were also misidentified. This suggests that decision-bound strategy analysis, which is commonly used as a manipulation check in COVIS studies, may not be valid for determining participants' category learning strategies. \par
	
\subsubsection{A multidimensional space}

One of the key insights from the Sloustky statistical density approach is that category structure lies on a spectrum. By using the formulas described in \citet{Sloutsky2010}, researchers can construct stimuli that are highly dense, highly sparse, or somewhere in between. Other researchers have pointed out similar in-between categories. \citet{Lupyan2013a} conducted a series of experiments showing that seemingly rule-based categories like grandmothers or even numbers exhibit typicality effects usually associated with probabilistic, similarity-based categories. This provides further support for the continuous nature of category structure. Thus, a dual-systems model with two systems each set up to learn a different type of category structure begins to fall apart; how would the hypothesis-testing and associative systems deal with one of these blended categories?  The verbal/nonverbal approach suggests that the two systems would run in parallel, and the system with the quickest answer or the strongest evidence would make the final category decision \citep{Minda2010}. 

\subsection{Conclusions and directions for future research}


\end{document}

