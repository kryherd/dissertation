\documentclass[../dissertation.tex]{subfiles}
 
\begin{document}

\section{General Discussion}

The goal of this dissertation was to closely examine the role of language in a dual-systems model of category learning. In this section I will review the findings of the current study and use them to reconsider my hypotheses and the overarching dual-systems framework for category learning.

\subsection{Language in a dual-systems model: summary of results}


\subsubsection{Language and the interaction between two systems}
In Experiment 1, I tested whether the order in which an individual engages different category learning systems affects learning in those systems. I also tested whether any observed order effects vary according to general language ability. I predicted that switching from the hypothesis-testing system to the associative system would produce a cost, while switching from associative to hypothesis-testing would not. This prediction was based on prior research showing dominance of the hypothesis-testing system \citep{Erickson2008, Ashby2010}. I also predicted that switch costs would be higher in individuals with poorer language ability, since my prior research showed that low-language individuals had difficulty switching away from suboptimal learning strategies \citep{Ryherd2019}. \par 
Neither of these hypotheses were supported by the data. Instead of poorer performance in second blocks, which would indicate a switch cost, all order effects were driven by better performance in second blocks, a result more consistent with a learning effect. In addition, there was no interaction between language ability and order in any of the three analyses. Thus, the observed learning effect did not depend on an individual's language ability.

\subsubsection{Individual differences and dual-systems category learning}

	In the first analysis of Experiment 2, I investigated whether individual differences in vocabulary and executive function would predict performance on three category learning tasks. I expected to see that vocabulary, acting as a measure of labeling ability, would be related to performance on associative category learning blocks, while executive function would be related to performance on hypothesis-testing blocks. I expected these relationships to hold across all three category learning tasks. \par
	The results did not support these hypotheses. First, performance in each category learning task was related to different individual difference measures. Vocabulary showed significant effects in the Ashby category learning task, while planning affected performance in the Sloutsky statistical density and taxonomic/thematic tasks. Further, the relationship between planning and performance differed for the Sloutsky and taxonomic/thematic tasks. \par 
	In addition, most of the relationships I found did not match the overall predictions. While I expected to see a positive relationship between vocabulary and associative learning and no relationship for hypothesis-testing learning, it seemed to facilitate hypothesis-testing learning and impair associative learning in the Ashby perceptual category task. Further, higher planning skill was specifically related to poorer performance in the associative block of the Sloutsky task. The only predicted relationship was in the taxonomic/thematic task, where planning (an executive function measure) was positively related to performance in the hypothesis-testing but not the associative block.
	
\subsubsection{Levels of processing and the dual-systems model}

The second analysis in Experiment 2 was the first to directly compare performance across three different paradigms designed to test dual-systems approaches to category learning. These approaches have considerable theoretical similarities (see Chapter \ref{intro}); as such, it is conceivable that the experimental tasks used to test these approaches measure the same type of processing. I tested subjects on the three tasks and hypothesized that the pattern of performance would be similar across all three. That is, if participants showed higher accuracy on the associative block than the hypothesis-testing block of one task, they should show the same pattern for the other two tasks. \par
This hypothesis was not supported by the results. In accuracy, participants showed significant differences between the associative and hypothesis-testing blocks for the Ashby perceptual category learning task, but no block differences for the Sloutsky statistical density task or the taxonomic/thematic task. In reaction time, the Ashby and Sloutsky tasks showed similar patterns, while the taxonomic/thematic task was the reverse. Together, the results suggest that these three tasks as they are typically administered are not comparable and do not engage the same processing.


\subsection{Rethinking the dual-systems model of categorization}

	In the introduction to this document, I drew parallels across six different approaches to category learning. Each approach split category learning and categorization in two. Some categories were probabilistic, with fuzzy boundaries for inclusion. Others were based on clearly-defined rules. Many of these approaches proposed a dual-systems model for learning these two types of categories, positing a different system specifically tuned for each category type's structure. I combined these different approaches to suggest a more unified dual-systems theory of category learning. However, the experiments described above do not support this combined theory. Rather than showing similarities across the different approaches, these experiments revealed different patterns of performance for each task as well as different relationships with individual difference measures. Thus, the theoretical similarities seen across these approaches are not apparent in the data. This casts doubt on a single unified dual-systems theory of category learning. Indeed, this investigation is not the first to question dual-systems models.

\subsubsection{Existing critiques of dual-systems models}

COVIS is the main dual-systems model some researchers have argued against, which is likely due to its prominence and popularity. While many studies have shown dissociations between the two category learning systems proposed in COVIS, some more recent studies have made strong claims against the existing evidence. For example, one paper points out that common stimuli used in a COVIS paradigm are not sufficiently matched. Thus, the double dissociations seen in these paradigms may be due to stimulus characteristics rather than differential processing. For example, one study tested the effect of feedback using stimuli that were matched on participant error rates, category separation, and relevant dimensions \citep{Edmunds2015}. This study did not find a difference between category type, a finding in opposition to previous studies with less carefully-matched stimuli \citep{Ashby2002, Maddox2003}. \par 
	Another critique of the COVIS framework is its assumption that items learned by the associative system are learned nonverbally and implicitly, and thus are not available to the conscious mind. To test this assumption, another study tested recognition memory for exemplars of rule-based (hypothesis-testing) and information-integration (associative) stimuli after the categories were learned \citep{Edmunds2016}. Recognition memory is commonly assumed to test explicit memory \citep{Gabrieli1995}. If participants could reliably recognize exemplars from information-integration categories, it would be unlikely that these items were being learned truly implicitly. In fact, \citet{Edmunds2016} found that participants not only were able to recognize information-integration (associative) stimuli at an above-chance rate, they were also more accurate at recognizing information-integration (associative) stimuli than rule-based (hypothesis-testing) stimuli. This suggests that instances that should have been learned using the associative system implicitly were at least available to explicit memory after learning. Further support for this critique comes from a study showing that participants produced verbal reports of their learning strategies that matched model-based strategy determination \cite{Edmunds2015}. Thus, participants were able to access both the items they had learned as well as the method they had learned for categorization.  \par 
	A third critique of COVIS centers on the previously-mentioned mathematical models, which are used to verify whether an individual is using associative or hypothesis-testing strategies to learn categories. Decision-bound strategy analysis fits different decision boundary models to the category responses made by participants to determine their learning strategies \citep{Maddox1993}. However, a recent study used simulations to test the validity of this type of analysis \citep{Edmunds2018}. The authors created simulated participants who used either associative or hypothesis-testing strategies, and then ran decision-bound strategy analysis on the simulated data. They found that over a third of simulated participants using a hypothesis-testing strategy were misidentified, while almost all simulated participants using an associative strategies were also misidentified. This suggests that decision-bound strategy analysis, which is commonly used as a manipulation check in COVIS studies, may not be valid for determining participants' category learning strategies. \par
	The existing critiques of COVIS serve as an initial jumping-off point for reconsidering dual-systems models of category learning. While the introduction to this document highlighted the considerable theoretical overlap among multiple frameworks, it did not examine the assumptions each framework makes about the process of category learning. Careful consideration and empirical investigation of these assumptions may serve to help us better understand how to fit these approaches together by elucidating the specific phenomena and processes each approach attempts to explain. One more obvious aspect that has been largely overlooked in this study so far is the way in which depth of processing interacts with category structure.
	
\subsubsection{A multidimensional space}
	In this section, I will argue that carving categories and category learning into discrete chunks is an oversimplification, both in terms of category structure and the systems used to learn them. Instead, I will suggest that category learning should be considered in terms of two continuous measures. First, I suggest that category structure is continuous, ranging from highly rule-based categories to strongly similarity-based ones with blended categories in between. Next, I highlight how depth of processing should be considered when conducting investigations of category learning. \par
	One of the key insights from the Sloustky statistical density approach is that category structure lies on a spectrum. By using the formulas described in \citet{Sloutsky2010}, researchers can construct stimuli that are highly dense, highly sparse, or somewhere in between. Other researchers have pointed out similar in-between categories. \citet{Lupyan2013a} conducted a series of experiments showing that seemingly rule-based categories like grandmothers or even numbers exhibit typicality effects usually associated with probabilistic, similarity-based categories. This suggests a spectrum of category structure, where the presence or even relative importance of category rules interacts with category typicality and other hallmarks of similarity-based categories. Categories at either end are the ones described by most dual-systems approaches: rule-based vs. similarity-based; low-dimensional vs. high-dimensional; dense vs. sparse, etc. However, these models are not yet equipped to deal with categories whose structure lies in the middle of the spectrum.  \par 
	Thus, we see that a dual-systems model with two systems each set up to learn a different type of category structure begins to fall apart. How would the hypothesis-testing and associative systems deal with one of these blended categories?  The verbal/nonverbal approach suggests that the two systems would run in parallel, and the system with the quickest answer or the strongest evidence would make the final category decision \citep{Minda2010}. In fact, overlapping brain regions have been found to support both associative and hypothesis-testing category learning, suggesting common or at least parallel processing \citep{Carpenter2016}. \par 
	Another way to characterize the experiments conducted above is in terms of depth of processing. Some neurobiological research has shown that visual processing of objects takes place in occipital regions, while processing the semantic features of objects occurs in temporal pole and parahippocampal regions \citep{Man2018}. This suggests that stimuli that are primarily visual with little to no meaning (like sine-wave gratings) would be processed differently than taxonomically or thematically related items. In addition, new research shows that when retrieving the meaning of a word, we access both perceptual and conceptual information on a similar time frame \citep{Borghesani2019}. This suggests that semantic information is available at the same time as perceptual information. Thus, categorization involving relations among items (e.g., taxonomic and thematic judgments) may involve the integration of more information sources than perceptual category learning in a similar time frame. \par 
	Clearly, more research is needed to disentangle the effects of category structure and depth of processing on category learning and the processes that support it. By considering both dimensions in a continuous manner, future research will be able to better understand the relationships that exist among the many different approaches to category learning. 
	
\subsection{Conclusions and directions for future research}

	For this document, I started with a fairly simple premise: there are lots of approaches to category learning, and they all can be mapped on to a single framework. I identified a dual-systems model for category learning that drew elements from six different approaches and proposed hypotheses about how language could play a role in each system. Then, I conducted two experiments and three analyses to test these hypotheses and the broader framework. Despite considerable theoretical overlap, I found that each approach showed varying results. Participants showed different patterns of performance for the two category learning systems in each approach. Different individual difference measures were related to each approach, and even when a single measure was common across multiple approaches, the relationship was different. Together, these results suggest that the simple framework proposed in my introduction is not a sufficient unifying theory for category learning. \par 
	This investigation strongly highlighted the complexity involved in understanding category learning. There is a reason that reviews of COVIS explicitly constrain the domain of their explanation (perceptual categories) at the outset. It is possible that no single theory can explain category learning at all levels of processing. However, the move from a discrete view of category systems and structure to something more continuous can only benefit future investigations. By adopting a continuous understanding of category structure, researchers may find additional metrics to describe their categories of interest. In addition, this investigation should highlight the importance of thoroughly considering the assumptions used in any theoretical approach to category learning. For example, many of the claims made by Sloutsky and the statistical density approach rely on a empirical investigations involving a mathematically-determined measure of category density. However, category density is calculated using a constant for attentional weighting that is assumed to be the same for all features. Without careful testing of this assumption, we cannot truly determine the category density, which makes interpretation of statistical density tasks more difficult. \par 
	

\end{document}

